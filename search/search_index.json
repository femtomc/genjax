{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GenJAX","text":"<p>A JAX-based probabilistic programming language with programmable inference</p> <ul> <li> <p>:material-rocket-launch-outline: Fast &amp; Scalable   Built on JAX for automatic differentiation, JIT compilation, and hardware acceleration</p> </li> <li> <p>:material-code-braces: Pythonic API   Write probabilistic models using familiar Python syntax with the <code>@gen</code> decorator</p> </li> <li> <p>:material-tune: Programmable Inference   Customize inference algorithms or use built-in MCMC, SMC, and variational methods</p> </li> <li> <p>:material-vector-combine: Composable Models   Build complex models from simple components using combinators</p> </li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import jax.numpy as jnp\nfrom genjax import gen, normal, bernoulli\n\n@gen\ndef coin_flipping_model(n_flips):\n    # Prior on coin bias\n    bias = normal(0.5, 0.1) @ \"bias\"\n\n    # Generate flips\n    flips = []\n    for i in range(n_flips):\n        flip = bernoulli(bias) @ f\"flip_{i}\"\n        flips.append(flip)\n\n    return jnp.array(flips)\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#generative-function-interface-gfi","title":"\ud83c\udfaf Generative Function Interface (GFI)","text":"<p>GenJAX implements the Generative Function Interface, providing a unified API for:</p> <ul> <li>Forward sampling (<code>simulate</code>)</li> <li>Density evaluation (<code>assess</code>)</li> <li>Constrained generation (<code>generate</code>)</li> <li>Trace updates (<code>update</code>)</li> <li>Selective regeneration (<code>regenerate</code>)</li> </ul>"},{"location":"#built-in-inference-algorithms","title":"\ud83d\udd27 Built-in Inference Algorithms","text":"<ul> <li>MCMC: Metropolis-Hastings, HMC, MALA</li> <li>Sequential Monte Carlo: Particle filtering, SMC samplers</li> <li>Variational Inference: Black-box VI with gradient estimators</li> </ul>"},{"location":"#jax-integration","title":"\ud83d\ude80 JAX Integration","text":"<ul> <li>JIT compilation for performance</li> <li>Automatic differentiation for gradient-based inference</li> <li>Hardware acceleration (GPU/TPU)</li> <li>Functional programming paradigm</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install genjax\n</code></pre> <p>Or with conda/mamba:</p> <pre><code>conda install -c conda-forge genjax\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p>:material-school: Tutorial   Learn the basics of GenJAX with hands-on examples</p> </li> <li> <p>:material-api: API Reference   Detailed documentation of all GenJAX components</p> </li> <li> <p>:material-flask: Examples   Explore real-world applications and case studies</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>GenJAX is released under the MIT License. See the LICENSE file for details.</p>"},{"location":"api/distributions/","title":"Distributions","text":"<p>GenJAX provides a comprehensive set of probability distributions that implement the Generative Function Interface. All distributions can be used directly in <code>@gen</code> functions with the <code>@</code> addressing operator.</p>"},{"location":"api/distributions/#continuous-distributions","title":"Continuous Distributions","text":""},{"location":"api/distributions/#normal-gaussian","title":"Normal (Gaussian)","text":"<pre><code>from genjax import normal\n\n# Standard normal\nx = normal(0, 1) @ \"x\"\n\n# With parameters\ny = normal(mu=5.0, sigma=2.0) @ \"y\"\n\n# In a model\n@gen\ndef model():\n    mean = normal(0, 10) @ \"mean\"\n    data = normal(mean, 1) @ \"data\"\n    return data\n</code></pre> <p>Parameters: - <code>mu</code>: Mean (location parameter) - <code>sigma</code>: Standard deviation (scale parameter, must be positive)</p>"},{"location":"api/distributions/#beta","title":"Beta","text":"<pre><code>from genjax import beta\n\n# Beta(2, 5)\np = beta(2, 5) @ \"probability\"\n\n# Uniform prior (Beta(1, 1))\nuniform_p = beta(1, 1) @ \"uniform\"\n</code></pre> <p>Parameters: - <code>alpha</code>: First shape parameter (must be positive) - <code>beta</code>: Second shape parameter (must be positive)</p> <p>Support: [0, 1]</p>"},{"location":"api/distributions/#gamma","title":"Gamma","text":"<pre><code>from genjax import gamma\n\n# Gamma(shape=2, rate=1)\nx = gamma(2, 1) @ \"x\"\n\n# For inverse scale parameterization\n# Gamma(shape=\u03b1, scale=1/\u03b2) has mean \u03b1/\u03b2\nprecision = gamma(1, 1) @ \"precision\"\n</code></pre> <p>Parameters: - <code>shape</code>: Shape parameter \u03b1 (must be positive) - <code>rate</code>: Rate parameter \u03b2 (must be positive)</p> <p>Support: (0, \u221e)</p>"},{"location":"api/distributions/#exponential","title":"Exponential","text":"<pre><code>from genjax import exponential\n\n# Exponential with rate 2.0\nwaiting_time = exponential(2.0) @ \"wait\"\n\n# Mean = 1/rate, so rate=0.1 gives mean=10\nlong_wait = exponential(0.1) @ \"long_wait\"\n</code></pre> <p>Parameters: - <code>rate</code>: Rate parameter \u03bb (must be positive)</p> <p>Support: [0, \u221e)</p>"},{"location":"api/distributions/#uniform","title":"Uniform","text":"<pre><code>from genjax import uniform\n\n# Uniform on [0, 1]\nu = uniform(0, 1) @ \"u\"\n\n# Uniform on [-5, 5]  \nx = uniform(-5, 5) @ \"x\"\n</code></pre> <p>Parameters: - <code>low</code>: Lower bound - <code>high</code>: Upper bound (must be greater than low)</p> <p>Support: [low, high]</p>"},{"location":"api/distributions/#dirichlet","title":"Dirichlet","text":"<pre><code>from genjax import dirichlet\nimport jax.numpy as jnp\n\n# Symmetric Dirichlet\nprobs = dirichlet(jnp.ones(3)) @ \"probs\"\n\n# Asymmetric Dirichlet\nalphas = jnp.array([1.0, 2.0, 3.0])\nweights = dirichlet(alphas) @ \"weights\"\n</code></pre> <p>Parameters: - <code>alpha</code>: Concentration parameters (array, all elements must be positive)</p> <p>Support: Simplex (sums to 1)</p>"},{"location":"api/distributions/#multivariate-normal","title":"Multivariate Normal","text":"<pre><code>from genjax import multivariate_normal\nimport jax.numpy as jnp\n\n# 2D standard normal\nx = multivariate_normal(\n    jnp.zeros(2), \n    jnp.eye(2)\n) @ \"x\"\n\n# With correlation\nmean = jnp.array([1.0, 2.0])\ncov = jnp.array([[1.0, 0.5], \n                 [0.5, 2.0]])\ny = multivariate_normal(mean, cov) @ \"y\"\n</code></pre> <p>Parameters: - <code>mean</code>: Mean vector - <code>cov</code>: Covariance matrix (must be positive definite)</p>"},{"location":"api/distributions/#discrete-distributions","title":"Discrete Distributions","text":""},{"location":"api/distributions/#bernoulli","title":"Bernoulli","text":"<pre><code>from genjax import bernoulli\n\n# Fair coin\ncoin = bernoulli(0.5) @ \"coin\"\n\n# Biased coin\nbiased = bernoulli(0.7) @ \"biased\"\n\n# In a model\n@gen\ndef coin_flips(n):\n    p = beta(1, 1) @ \"bias\"\n    flips = []\n    for i in range(n):\n        flip = bernoulli(p) @ f\"flip_{i}\"\n        flips.append(flip)\n    return flips\n</code></pre> <p>Parameters: - <code>p</code>: Probability of success (must be in [0, 1])</p> <p>Support: {0, 1} (False, True)</p>"},{"location":"api/distributions/#categorical","title":"Categorical","text":"<pre><code>from genjax import categorical\nimport jax.numpy as jnp\n\n# Three categories with equal probability\nx = categorical(jnp.ones(3) / 3) @ \"x\"\n\n# With specified probabilities\nprobs = jnp.array([0.1, 0.3, 0.6])\ncategory = categorical(probs) @ \"category\"\n\n# In a mixture model\n@gen\ndef mixture(n_components):\n    weights = dirichlet(jnp.ones(n_components)) @ \"weights\"\n\n    # Assign to components\n    assignments = []\n    for i in range(n_data):\n        z = categorical(weights) @ f\"z_{i}\"\n        assignments.append(z)\n    return assignments\n</code></pre> <p>Parameters: - <code>probs</code>: Probability vector (must sum to 1)</p> <p>Support: {0, 1, ..., len(probs)-1}</p>"},{"location":"api/distributions/#poisson","title":"Poisson","text":"<pre><code>from genjax import poisson\n\n# Poisson with rate 3.0\ncount = poisson(3.0) @ \"count\"\n\n# Modeling count data\n@gen\ndef count_model(exposure):\n    rate = gamma(2, 1) @ \"rate\"\n    counts = []\n    for i in range(len(exposure)):\n        count = poisson(rate * exposure[i]) @ f\"count_{i}\"\n        counts.append(count)\n    return counts\n</code></pre> <p>Parameters: - <code>rate</code>: Rate parameter \u03bb (must be positive)</p> <p>Support: {0, 1, 2, ...}</p>"},{"location":"api/distributions/#flip","title":"Flip","text":"<p>Alias for Bernoulli with boolean output:</p> <pre><code>from genjax import flip\n\n# Equivalent to bernoulli but more intuitive for booleans\nif flip(0.8) @ \"success\":\n    reward = normal(10, 1) @ \"reward\"\nelse:\n    reward = normal(0, 1) @ \"reward\"\n</code></pre>"},{"location":"api/distributions/#using-distributions-outside-gen-functions","title":"Using Distributions Outside <code>@gen</code> Functions","text":"<p>All distributions implement the full GFI and can be used directly:</p> <pre><code># Direct sampling (requires explicit key)\nfrom genjax import seed\nimport jax.random as random\n\nkey = random.PRNGKey(0)\nsample = seed(normal.simulate)(key, mu=0, sigma=1)\n\n# Log probability\nlog_prob, _ = normal.assess({\"value\": 1.5}, mu=0, sigma=1)\n\n# Generate with constraints  \ntrace, weight = normal.generate({\"value\": 2.0}, mu=0, sigma=1)\n</code></pre>"},{"location":"api/distributions/#custom-distributions","title":"Custom Distributions","text":"<p>You can create custom distributions by implementing the GFI:</p> <pre><code>from genjax import Distribution\nimport jax.numpy as jnp\n\nclass Laplace(Distribution):\n    \"\"\"Laplace (double exponential) distribution.\"\"\"\n\n    def sample(self, key, loc, scale):\n        u = random.uniform(key, minval=-0.5, maxval=0.5)\n        return loc - scale * jnp.sign(u) * jnp.log(1 - 2 * jnp.abs(u))\n\n    def log_density(self, value, loc, scale):\n        return -jnp.log(2 * scale) - jnp.abs(value - loc) / scale\n\n# Use in a model\n@gen\ndef robust_regression(x):\n    # Laplace errors for robust regression\n    intercept = normal(0, 10) @ \"intercept\"\n    slope = normal(0, 5) @ \"slope\"\n\n    errors = []\n    for i in range(len(x)):\n        # Would need to register as GenJAX distribution\n        error = custom_laplace(0, 1) @ f\"error_{i}\"\n        errors.append(error)\n\n    return intercept + slope * x + jnp.array(errors)\n</code></pre>"},{"location":"api/distributions/#distribution-parameters","title":"Distribution Parameters","text":""},{"location":"api/distributions/#shape-conventions","title":"Shape Conventions","text":"<ul> <li>Scalar parameters: Single values (e.g., <code>normal(0, 1)</code>)</li> <li>Vector parameters: Use JAX arrays (e.g., <code>dirichlet(jnp.ones(3))</code>)</li> <li>Matrix parameters: For multivariate distributions (e.g., <code>multivariate_normal(mean, cov)</code>)</li> </ul>"},{"location":"api/distributions/#broadcasting","title":"Broadcasting","text":"<p>GenJAX distributions support JAX broadcasting:</p> <pre><code># Sample multiple values with different means\nmeans = jnp.array([0.0, 1.0, 2.0])\nx = normal(means, 1.0) @ \"x\"  # Shape: (3,)\n\n# Different means and sigmas\nsigmas = jnp.array([0.5, 1.0, 2.0])  \ny = normal(means, sigmas) @ \"y\"  # Shape: (3,)\n</code></pre>"},{"location":"api/distributions/#common-patterns","title":"Common Patterns","text":""},{"location":"api/distributions/#hierarchical-models","title":"Hierarchical Models","text":"<pre><code>@gen\ndef hierarchical():\n    # Global parameters\n    global_mean = normal(0, 10) @ \"global_mean\"\n    global_std = gamma(1, 1) @ \"global_std\"\n\n    # Group-level parameters\n    group_means = []\n    for g in range(n_groups):\n        group_mean = normal(global_mean, global_std) @ f\"group_{g}_mean\"\n        group_means.append(group_mean)\n\n    # Observations\n    for g in range(n_groups):\n        for i in range(n_obs_per_group):\n            obs = normal(group_means[g], 1.0) @ f\"obs_{g}_{i}\"\n</code></pre>"},{"location":"api/distributions/#prior-predictive-sampling","title":"Prior Predictive Sampling","text":"<pre><code>@gen\ndef model():\n    # Priors\n    theta = beta(2, 2) @ \"theta\"\n\n    # Likelihood\n    successes = 0\n    for i in range(n_trials):\n        if bernoulli(theta) @ f\"trial_{i}\":\n            successes += 1\n\n    return successes\n\n# Sample from prior predictive\ntrace = model.simulate()\nprior_predictive_sample = trace.get_retval()\n</code></pre>"},{"location":"api/distributions/#posterior-predictive","title":"Posterior Predictive","text":"<pre><code># After inference, use posterior samples\nposterior_trace = inference_algorithm(model, data)\ntheta_posterior = posterior_trace.get_choices()[\"theta\"]\n\n# Generate new predictions\n@gen\ndef predictive(theta):\n    predictions = []\n    for i in range(n_future):\n        pred = bernoulli(theta) @ f\"pred_{i}\"\n        predictions.append(pred)\n    return predictions\n\npred_trace = predictive.simulate(theta=theta_posterior)\n</code></pre>"},{"location":"api/generative-functions/","title":"Generative Functions","text":"<p>Generative functions are the core abstraction in GenJAX. They represent probabilistic computations that can be executed, scored, and manipulated through a unified interface.</p>"},{"location":"api/generative-functions/#creating-generative-functions","title":"Creating Generative Functions","text":""},{"location":"api/generative-functions/#the-gen-decorator","title":"The <code>@gen</code> Decorator","text":"<p>Transform regular Python functions into generative functions:</p> <pre><code>from genjax import gen, normal, bernoulli\n\n@gen\ndef weather_model(temp_yesterday):\n    # Sample today's temperature\n    temp_today = normal(temp_yesterday, 5.0) @ \"temp\"\n\n    # Determine if it rains based on temperature\n    rain_prob = 1 / (1 + jnp.exp(0.1 * (temp_today - 20)))\n    rains = bernoulli(rain_prob) @ \"rains\"\n\n    return {\"temperature\": temp_today, \"rains\": rains}\n</code></pre>"},{"location":"api/generative-functions/#addressing-random-choices","title":"Addressing Random Choices","text":"<p>Use the <code>@</code> operator to assign addresses to random choices:</p> <pre><code>@gen\ndef model():\n    # Simple address\n    x = normal(0, 1) @ \"x\"\n\n    # Hierarchical addresses\n    for i in range(3):\n        # Creates addresses: \"group_0\", \"group_1\", \"group_2\"\n        group_mean = normal(0, 1) @ f\"group_{i}\"\n\n        for j in range(5):\n            # Creates: \"obs_0_0\", \"obs_0_1\", ..., \"obs_2_4\"\n            obs = normal(group_mean, 0.1) @ f\"obs_{i}_{j}\"\n</code></pre> <p>Avoid Address Collisions</p> <p>Each address at the same level must be unique. GenJAX will raise an error if you reuse addresses:</p> <pre><code>@gen\ndef bad_model():\n    x = normal(0, 1) @ \"x\"\n    y = normal(1, 1) @ \"x\"  # Error: address \"x\" already used!\n</code></pre>"},{"location":"api/generative-functions/#the-generative-function-interface","title":"The Generative Function Interface","text":"<p>All generative functions implement these methods:</p>"},{"location":"api/generative-functions/#simulate","title":"simulate","text":"<p>Forward sampling from the generative function:</p> <pre><code># Without arguments\ntrace = model.simulate()\n\n# With arguments\ntrace = weather_model.simulate(temp_yesterday=25.0)\n\n# Access the trace\nchoices = trace.get_choices()\nreturn_value = trace.get_retval()\n</code></pre>"},{"location":"api/generative-functions/#assess","title":"assess","text":"<p>Evaluate the log probability density at given choices:</p> <pre><code>choices = {\n    \"temp\": 22.0,\n    \"rains\": True\n}\n\nlog_prob, retval = weather_model.assess(choices, temp_yesterday=25.0)\n# log_prob = log p(temp=22.0, rains=True | temp_yesterday=25.0)\n</code></pre>"},{"location":"api/generative-functions/#generate","title":"generate","text":"<p>Generate a trace with some choices constrained:</p> <pre><code># Observe that it rained\nconstraints = {\"rains\": True}\n\ntrace, weight = weather_model.generate(constraints, temp_yesterday=25.0)\n# weight = log p(rains=True, temp) / q(temp | rains=True)\n</code></pre> <p>The weight is the incremental importance weight, useful for: - Importance sampling - Particle filtering - MCMC acceptance probabilities</p>"},{"location":"api/generative-functions/#update","title":"update","text":"<p>Update an existing trace with new constraints:</p> <pre><code># Original trace\ntrace = weather_model.simulate(temp_yesterday=25.0)\n\n# Update with new observation\nnew_constraints = {\"rains\": False}\nnew_trace, weight, discard = weather_model.update(\n    trace, \n    new_constraints, \n    temp_yesterday=26.0  # Can also change arguments\n)\n</code></pre>"},{"location":"api/generative-functions/#regenerate","title":"regenerate","text":"<p>Selectively regenerate parts of a trace:</p> <pre><code>from genjax import sel\n\n# Regenerate only the temperature\nselection = sel(\"temp\")\nnew_trace, weight, discard = weather_model.regenerate(\n    trace,\n    selection,\n    temp_yesterday=25.0\n)\n</code></pre>"},{"location":"api/generative-functions/#composing-generative-functions","title":"Composing Generative Functions","text":""},{"location":"api/generative-functions/#calling-other-generative-functions","title":"Calling Other Generative Functions","text":"<pre><code>@gen\ndef prior():\n    mean = normal(0, 10) @ \"mean\"\n    std = gamma(1, 1) @ \"std\"\n    return mean, std\n\n@gen\ndef model(n_obs):\n    # Call another generative function\n    mean, std = prior() @ \"prior\"\n\n    # Use the results\n    observations = []\n    for i in range(n_obs):\n        obs = normal(mean, std) @ f\"obs_{i}\"\n        observations.append(obs)\n\n    return jnp.array(observations)\n</code></pre>"},{"location":"api/generative-functions/#using-fixed-values","title":"Using Fixed Values","text":"<p>Wrap deterministic values to preserve them during trace operations:</p> <pre><code>from genjax import Fixed\n\n@gen\ndef model_with_fixed():\n    # This value won't be regenerated\n    fixed_param = Fixed(1.0) @ \"param\"\n\n    # This can be regenerated\n    x = normal(fixed_param, 1.0) @ \"x\"\n\n    return x\n</code></pre>"},{"location":"api/generative-functions/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"api/generative-functions/#mixture-models","title":"Mixture Models","text":"<pre><code>@gen\ndef mixture_model(data):\n    # Mixture weights\n    weights = dirichlet(jnp.ones(3)) @ \"weights\"\n\n    # Component parameters\n    means = []\n    for k in range(3):\n        mean = normal(0, 10) @ f\"mean_{k}\"\n        means.append(mean)\n\n    # Assign data to components\n    for i, datum in enumerate(data):\n        component = categorical(weights) @ f\"z_{i}\"\n        obs = normal(means[component], 1.0) @ f\"obs_{i}\"\n</code></pre>"},{"location":"api/generative-functions/#recursive-models","title":"Recursive Models","text":"<pre><code>@gen\ndef geometric(p, max_depth=100):\n    \"\"\"Sample from geometric distribution recursively.\"\"\"\n    flip = bernoulli(p) @ f\"flip_0\"\n\n    if flip:\n        return 0\n    else:\n        # Recursive call\n        rest = geometric(p, max_depth-1) @ \"rest\"\n        return 1 + rest\n</code></pre>"},{"location":"api/generative-functions/#state-space-models","title":"State Space Models","text":"<pre><code>@gen\ndef state_space_model(T, observations):\n    # Initial state\n    state = normal(0, 1) @ \"state_0\"\n\n    states = [state]\n    for t in range(1, T):\n        # State transition\n        state = normal(state, 0.1) @ f\"state_{t}\"\n        states.append(state)\n\n        # Observation\n        if observations[t] is not None:\n            obs = normal(state, 0.5) @ f\"obs_{t}\"\n            # Could add constraint: obs == observations[t]\n\n    return jnp.array(states)\n</code></pre>"},{"location":"api/generative-functions/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive addresses: Make debugging easier with meaningful names</li> <li>Avoid address collisions: Each address at the same level must be unique</li> <li>Minimize Python loops: Use JAX/GenJAX combinators when possible</li> <li>Type annotations: Help with debugging and documentation</li> <li>Return structured data: Return dictionaries or named tuples for clarity</li> </ol>"},{"location":"api/generative-functions/#common-pitfalls","title":"Common Pitfalls","text":"<p>Python Control Flow in JAX</p> <p>Avoid Python <code>if</code>/<code>for</code> statements when you need JAX compilation:</p> <pre><code># Bad - won't work with JAX transformations\n@gen\ndef bad_model(n):\n    for i in range(n):  # Python loop with traced value!\n        x = normal(0, 1) @ f\"x_{i}\"\n\n# Good - use Scan combinator\nfrom genjax import Scan\n\n@gen\ndef step(carry, i):\n    x = normal(0, 1) @ \"x\"\n    return carry, x\n\nmodel = Scan(step, const(n))\n</code></pre> <p>Performance Tips</p> <ul> <li>Use <code>Fixed</code> for values that don't need regeneration</li> <li>Batch operations with <code>vmap</code> instead of loops</li> <li>Prefer built-in distributions over custom implementations</li> </ul>"},{"location":"api/overview/","title":"Core API Overview","text":"<p>GenJAX provides a powerful and composable API for probabilistic programming. The core concepts are:</p>"},{"location":"api/overview/#generative-functions","title":"Generative Functions","text":"<p>The fundamental abstraction in GenJAX is the generative function - a probabilistic program that can be executed, scored, and manipulated through the Generative Function Interface (GFI).</p>"},{"location":"api/overview/#the-gen-decorator","title":"The <code>@gen</code> Decorator","text":"<p>Transform Python functions into generative functions:</p> <pre><code>from genjax import gen, normal\n\n@gen\ndef my_model(x):\n    # Sample from distributions using @ for addressing\n    z = normal(0, 1) @ \"z\"\n    y = normal(z * x, 0.1) @ \"y\"\n    return y\n</code></pre>"},{"location":"api/overview/#addressing-with","title":"Addressing with <code>@</code>","text":"<p>The <code>@</code> operator assigns addresses to random choices, creating a hierarchical namespace:</p> <pre><code>@gen\ndef hierarchical_model():\n    # Top-level choice\n    global_mean = normal(0, 1) @ \"global_mean\"\n\n    # Nested choices\n    for i in range(3):\n        local_mean = normal(global_mean, 0.5) @ f\"group_{i}/mean\"\n        for j in range(5):\n            obs = normal(local_mean, 0.1) @ f\"group_{i}/obs_{j}\"\n</code></pre>"},{"location":"api/overview/#generative-function-interface-gfi","title":"Generative Function Interface (GFI)","text":"<p>Every generative function implements these core methods:</p>"},{"location":"api/overview/#simulateargs-trace","title":"<code>simulate(args...) -&gt; Trace</code>","text":"<p>Forward sampling from the model:</p> <pre><code>trace = model.simulate(x=2.0)\nchoices = trace.get_choices()  # {\"z\": 0.5, \"y\": 1.1}\nretval = trace.get_retval()    # 1.1\n</code></pre>"},{"location":"api/overview/#assesschoices-args-log_density-retval","title":"<code>assess(choices, args...) -&gt; (log_density, retval)</code>","text":"<p>Evaluate the log probability density:</p> <pre><code>choices = {\"z\": 0.5, \"y\": 1.0}\nlog_prob, retval = model.assess(choices, x=2.0)\n</code></pre>"},{"location":"api/overview/#generateconstraints-args-trace-weight","title":"<code>generate(constraints, args...) -&gt; (trace, weight)</code>","text":"<p>Generate a trace with some choices constrained:</p> <pre><code>constraints = {\"y\": 1.5}  # Fix observation\ntrace, weight = model.generate(constraints, x=2.0)\n# weight = log p(y=1.5, z) / q(z | y=1.5)\n</code></pre>"},{"location":"api/overview/#updatetrace-constraints-args-new_trace-weight-discard","title":"<code>update(trace, constraints, args...) -&gt; (new_trace, weight, discard)</code>","text":"<p>Update an existing trace with new constraints:</p> <pre><code>new_constraints = {\"y\": 2.0}\nnew_trace, weight, discard = model.update(trace, new_constraints, x=2.0)\n</code></pre>"},{"location":"api/overview/#regeneratetrace-selection-args-new_trace-weight-discard","title":"<code>regenerate(trace, selection, args...) -&gt; (new_trace, weight, discard)</code>","text":"<p>Selectively regenerate parts of a trace:</p> <pre><code>from genjax import sel\n\nselection = sel(\"z\")  # Regenerate only z\nnew_trace, weight, discard = model.regenerate(trace, selection, x=2.0)\n</code></pre>"},{"location":"api/overview/#traces","title":"Traces","text":"<p>Traces record the execution of generative functions:</p> <pre><code>trace = model.simulate(x=2.0)\n\n# Access components\nchoices = trace.get_choices()      # Random choices\nretval = trace.get_retval()        # Return value\nscore = trace.get_score()          # log(1/p(choices))\nargs = trace.get_args()            # Function arguments\ngen_fn = trace.get_gen_fn()        # Source generative function\n</code></pre>"},{"location":"api/overview/#distributions","title":"Distributions","text":"<p>Built-in probability distributions that implement the GFI:</p> <pre><code>from genjax import normal, beta, categorical, bernoulli\n\n# Continuous distributions\nx = normal(mu=0, sigma=1) @ \"x\"\np = beta(alpha=2, beta=2) @ \"p\"\n\n# Discrete distributions  \nk = categorical(probs=jnp.array([0.2, 0.3, 0.5])) @ \"k\"\nb = bernoulli(p=0.7) @ \"b\"\n</code></pre>"},{"location":"api/overview/#combinators","title":"Combinators","text":"<p>Higher-order generative functions for composition:</p>"},{"location":"api/overview/#mapvmap","title":"Map/Vmap","text":"<p>Vectorized execution:</p> <pre><code># Map model over multiple inputs\nvectorized = model.vmap()\ntraces = vectorized.simulate(jnp.array([1.0, 2.0, 3.0]))\n</code></pre>"},{"location":"api/overview/#scan","title":"Scan","text":"<p>Sequential execution with state threading:</p> <pre><code>from genjax import Scan, const\n\n@gen\ndef step(state, x):\n    new_state = normal(state + x, 0.1) @ \"state\"\n    return new_state, new_state\n\nscan_model = Scan(step, const(10))  # 10 steps\ntrace = scan_model.simulate(init_state=0.0, xs=jnp.ones(10))\n</code></pre>"},{"location":"api/overview/#cond","title":"Cond","text":"<p>Conditional execution:</p> <pre><code>from genjax import Cond\n\n@gen\ndef model_a():\n    return normal(0, 1) @ \"x\"\n\n@gen  \ndef model_b():\n    return normal(5, 2) @ \"x\"\n\ncond_model = Cond(model_a, model_b)\ntrace = cond_model.simulate(condition=True)  # Uses model_a\n</code></pre>"},{"location":"api/overview/#selections","title":"Selections","text":"<p>Target specific addresses for operations:</p> <pre><code>from genjax import sel, Selection, AllSel\n\n# Select specific addresses\ns1 = sel(\"x\")                    # Select \"x\"\ns2 = sel(\"group_0\", \"mean\")      # Select \"group_0/mean\"\n\n# Combine selections\ns_or = sel(\"x\") | sel(\"y\")       # Select x OR y\ns_and = sel(\"x\") &amp; sel(\"y\")      # Select x AND y (intersection)\ns_not = ~sel(\"x\")                # Select everything except x\n\n# Select all\ns_all = Selection(AllSel())      # Select all addresses\n</code></pre>"},{"location":"api/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Generative Functions in detail</li> <li>Explore available Distributions</li> <li>Understand Traces and their structure</li> <li>Master Combinators for model composition</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with GenJAX in minutes! This guide covers the essential concepts through practical examples.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<pre><code>pip install genjax\n</code></pre>"},{"location":"getting-started/quickstart/#your-first-model","title":"Your First Model","text":"<p>Let's start with a simple Bayesian coin flipping model:</p> <pre><code>import jax.numpy as jnp\nfrom genjax import gen, beta, bernoulli\n\n@gen\ndef coin_model(flips):\n    # Prior belief about coin fairness\n    bias = beta(2, 2) @ \"bias\"\n\n    # Generate predictions for each flip\n    predictions = []\n    for i, flip in enumerate(flips):\n        # Each flip is Bernoulli distributed\n        pred = bernoulli(bias) @ f\"flip_{i}\"\n        predictions.append(pred)\n\n    return jnp.array(predictions)\n</code></pre>"},{"location":"getting-started/quickstart/#running-inference","title":"Running Inference","text":""},{"location":"getting-started/quickstart/#forward-sampling","title":"Forward Sampling","text":"<p>Sample from the prior:</p> <pre><code># Simulate without any observations\ntrace = coin_model.simulate(flips=[None, None, None])\n\n# Extract the sampled bias\nbias_sample = trace.get_choices()[\"bias\"]\nprint(f\"Sampled bias: {bias_sample:.3f}\")\n\n# Get the predictions\npredictions = trace.get_retval()\nprint(f\"Predicted flips: {predictions}\")\n</code></pre>"},{"location":"getting-started/quickstart/#conditioning-on-data","title":"Conditioning on Data","text":"<p>Observe some coin flips and infer the bias:</p> <pre><code># Observed data: True = Heads, False = Tails\nobserved_flips = [True, True, False, True]\n\n# Condition on observations\nconstraints = {f\"flip_{i}\": flip for i, flip in enumerate(observed_flips)}\n\n# Generate a trace with these constraints\ntrace, weight = coin_model.generate(constraints, flips=observed_flips)\n\n# Extract posterior sample\nposterior_bias = trace.get_choices()[\"bias\"]\nprint(f\"Posterior bias sample: {posterior_bias:.3f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#using-mcmc","title":"Using MCMC","text":"<p>For more complex models, use Markov Chain Monte Carlo:</p> <pre><code>from genjax import mh, sel\nimport jax.random as random\n\n# Initialize with prior sample\nkey = random.PRNGKey(0)\ntrace = coin_model.simulate(flips=observed_flips)\n\n# Run Metropolis-Hastings\nselection = sel(\"bias\")  # Only update the bias\nchain = []\n\nfor i in range(1000):\n    key, subkey = random.split(key)\n    trace, accepted = mh(trace, selection, key=subkey)\n\n    if i % 100 == 0:  # Thin the chain\n        chain.append(trace.get_choices()[\"bias\"])\n\n# Analyze posterior\nposterior_mean = jnp.mean(jnp.array(chain))\nprint(f\"Posterior mean bias: {posterior_mean:.3f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#a-more-complex-example-linear-regression","title":"A More Complex Example: Linear Regression","text":"<pre><code>@gen\ndef linear_regression(x):\n    # Priors\n    slope = normal(0, 10) @ \"slope\"\n    intercept = normal(0, 10) @ \"intercept\"\n    noise = gamma(1, 1) @ \"noise\"\n\n    # Likelihood\n    y_values = []\n    for i in range(len(x)):\n        y = normal(intercept + slope * x[i], noise) @ f\"y_{i}\"\n        y_values.append(y)\n\n    return jnp.array(y_values)\n\n# Generate synthetic data\ntrue_slope = 2.0\ntrue_intercept = 1.0\nx_data = jnp.linspace(-2, 2, 20)\ny_data = true_intercept + true_slope * x_data + random.normal(key, shape=(20,)) * 0.5\n\n# Condition on observed y values\nconstraints = {f\"y_{i}\": y_data[i] for i in range(len(y_data))}\n\n# Use SMC for inference\nfrom genjax import init, resample\n\n# Initialize particles\nparticles = init(linear_regression, (x_data,), n_particles=100, constraints=constraints)\n\n# Resample based on weights\nparticles = resample(particles)\n\n# Extract posterior samples\nslopes = [p.trace.get_choices()[\"slope\"] for p in particles]\nprint(f\"Posterior mean slope: {jnp.mean(jnp.array(slopes)):.3f}\")\nprint(f\"True slope: {true_slope}\")\n</code></pre>"},{"location":"getting-started/quickstart/#key-concepts-summary","title":"Key Concepts Summary","text":"<ol> <li><code>@gen</code> decorator: Transforms functions into generative functions</li> <li><code>@</code> operator: Assigns addresses to random choices</li> <li>Traces: Record executions including choices and return values</li> <li>Constraints: Fix certain random choices for conditioning</li> <li>Inference: Various algorithms (MH, SMC, VI) for posterior inference</li> </ol>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Tutorial for in-depth examples</li> <li>Learn about Inference Algorithms</li> <li>Check out Advanced Examples</li> <li>Read the API Reference</li> </ul>"},{"location":"getting-started/quickstart/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: github.com/femtomc/genjax/issues</li> <li>Documentation: femtomc.github.io/genjax</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>Detailed API documentation coming soon.</p>"}]}