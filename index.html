<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="GenJAX ‚Äî a language and compiler for vectorized programmable probabilistic inference, built on JAX. POPL 2026." />
  <meta name="theme-color" content="#2c5aa0" />
  <title>GenJAX: Probabilistic Programming with Vectorized Programmable Inference</title>
  <link rel="stylesheet" href="static/css/style.css" />
  <link rel="icon" type="image/png" href="static/images/logo.png" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:wght@400;500&family=Source+Sans+Pro:wght@400;500;600;700&family=Source+Serif+Pro:wght@400;600&display=swap" rel="stylesheet">
  <script>
    window.MathJax = {
      tex: { 
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      svg: { fontCache: 'global' },
      startup: { typeset: false }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <script src="static/js/site.js" defer></script>
</head>
<body>

<a href="#main-content" class="skip-link">Skip to main content</a>

<!-- Navigation -->
<nav class="site-nav" role="navigation" aria-label="Main navigation">
  <div class="container nav-content">
    <div class="nav-brand"><a href="#">GenJAX</a></div>
    <div class="nav-links">
      <a href="#overview">Overview</a>
      <a href="#tutorial">Tutorial</a>
      <a href="#theory">Theory</a>
      <a href="#evaluation">Evaluation</a>
      <a href="#artifact">Artifact</a>
      <a href="#citation">Citation</a>
    </div>
  </div>
</nav>

<!-- Header -->
<header class="paper-header">
  <div class="container">
    <h1>Probabilistic Programming with<br>Vectorized Programmable Inference</h1>
    <p class="venue">POPL 2026 ¬∑ Proceedings of the ACM on Programming Languages (PACMPL), Vol. 10, Article 87</p>
  </div>
</header>

<!-- Authors -->
<section class="authors-section" aria-label="Authors">
  <div class="container">
    <div class="authors">
      <span class="author-name">McCoy R. Becker*</span>,
      <span class="author-name">Mathieu Huot*</span>,
      <span class="author-name">George Matheos</span>,
      <span class="author-name">Xiaoyan Wang</span>,
      <span class="author-name">Karen Chung</span>,
      <span class="author-name">Colin Smith</span>,
      <span class="author-name">Sam Ritchie</span>,
      <span class="author-name">Rif A. Saurous</span>,
      <span class="author-name">Alexander K. Lew</span>,
      <span class="author-name">Martin C. Rinard</span>,
      <span class="author-name">Vikash K. Mansinghka</span>
      <span class="equal-contrib">* Equal contribution</span>
    </div>
    <div class="affiliations">
      Massachusetts Institute of Technology ¬∑ Google ¬∑ Yale University
    </div>
  </div>
</section>

<main id="main-content">
<div class="container">

<!-- Quick Links -->
<section id="links" aria-label="Quick links">
  <div class="links-grid">
    <a href="static/paper.pdf" class="link-btn">
      <svg viewBox="0 0 24 24" fill="currentColor" aria-hidden="true"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8l-6-6zm-1 2l5 5h-5V4zM6 20V4h5v7h7v9H6z"/></svg>
      Paper (PDF)
    </a>
    <a href="https://dl.acm.org/doi/10.1145/3776729" class="link-btn">
      <svg viewBox="0 0 24 24" fill="currentColor" aria-hidden="true"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-1 17.93c-3.95-.49-7-3.85-7-7.93 0-.62.08-1.21.21-1.79L9 15v1a2 2 0 0 0 2 2v1.93zm6.9-2.54A1.99 1.99 0 0 0 16 16h-1v-3a1 1 0 0 0-1-1H8v-2h2a1 1 0 0 0 1-1V7h2a2 2 0 0 0 2-2v-.41A7.997 7.997 0 0 1 20 12c0 2.08-.8 3.97-2.1 5.39z"/></svg>
      ACM Digital Library
    </a>
    <a href="https://github.com/femtomc/genjax" class="link-btn">
      <svg viewBox="0 0 16 16" fill="currentColor" aria-hidden="true"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/></svg>
      Source Code
    </a>
    <a href="https://doi.org/10.5281/zenodo.17342547" class="link-btn">
      <svg viewBox="0 0 24 24" fill="currentColor" aria-hidden="true"><path d="M12 2L2 7v10l10 5 10-5V7L12 2zm0 2.18l7 3.5v7.64l-7 3.5-7-3.5V7.68l7-3.5z"/></svg>
      Zenodo Artifact
    </a>
  </div>
</section>

<!-- Abstract -->
<section id="abstract" aria-labelledby="abstract-heading">
  <h2 id="abstract-heading">Abstract <a href="#abstract" class="section-anchor" aria-label="Link to Abstract">#</a></h2>
  <div class="abstract">
    <p>
      We present GenJAX, a new language and compiler for vectorized programmable probabilistic inference.
      GenJAX integrates the vectorizing map (<code>vmap</code>) operation from array programming frameworks such as JAX
      into the programmable inference paradigm, enabling compositional vectorization of features such as
      probabilistic program traces, stochastic branching (for expressing mixture models), and programmable
      inference interfaces for writing custom probabilistic inference algorithms.
      We formalize vectorization as a source-to-source program transformation on a core calculus for
      probabilistic programming, and prove that it correctly vectorizes both modeling and inference operations.
      We have implemented our approach in the GenJAX language and compiler, and have empirically evaluated
      this implementation on several benchmarks and case studies. Our results show that our implementation
      supports a wide and expressive set of programmable inference patterns and delivers performance
      comparable to hand-optimized JAX code.
    </p>
  </div>
</section>

<!-- Teaser Figure -->
<section class="teaser" aria-label="Key result">
  <img src="static/images/curvefit_outlier_detection_comparison.png"
       alt="Progressive refinement of polynomial regression with programmable inference in GenJAX"
       loading="eager" />
  <p class="caption">
    <strong>Figure 1.</strong> From simple importance sampling to programmable inference.
    (Left) Importance sampling on a simple polynomial model produces a tight fit, but cannot identify outliers.
    (Center) Importance sampling on an enriched outlier model identifies likely outliers but has wide posterior uncertainty.
    (Right) A custom Gibbs + HMC kernel on the outlier model produces accurate posterior samples.
    Each step adds a new GenJAX capability: <code>vmap</code> for scaling inference, 
    stochastic branching for richer models, and programmable kernels for better posterior approximation.
  </p>
</section>

<!-- Overview Section (Common to both tracks) -->
<section id="overview" aria-labelledby="overview-heading">
  <h2 id="overview-heading">Overview <a href="#overview" class="section-anchor" aria-label="Link to Overview">#</a></h2>
  
  <p>
    Probabilistic programming has demonstrated remarkable effectiveness in applications ranging from
    3D perception and robotics to automated data analysis, particle physics, and cognitive modeling.
    These applications require sophisticated probabilistic reasoning and rely on
    <em>programmable inference</em> ‚Äî the ability to customize inference algorithms through proposals,
    kernels, and variational families. But fully realizing these benefits often requires substantial
    computational resources, as inference scales by increasing particles, chains, or likelihood evaluations.
  </p>
  
  <p>
    GenJAX addresses this by integrating <code>vmap</code> from JAX into probabilistic programming with
    programmable inference, enabling compositional vectorization of three key features:
  </p>
  
  <ul>
    <li><strong>Compositional vectorization</strong> of both modeling code (e.g., computing likelihoods over many datapoints)
      and inference code (e.g., evolving particle collections or MCMC chains in parallel).</li>
    <li><strong>Vectorized traces</strong> ‚Äî structured records of random choices that maintain an efficient
      struct-of-array layout under <code>vmap</code>, serving as the data lingua franca for Monte Carlo and variational inference.</li>
    <li><strong>Vectorized stochastic branching</strong> ‚Äî mixture models and regime-switching dynamics
      that maintain vectorization even when different elements take different branches.</li>
  </ul>
  
  <h3>Contributions</h3>
  <ol class="contributions">
    <li><strong>GenJAX: High-Performance Compiler.</strong> An open-source compiler that extends JAX and
      <code>vmap</code> to support programmable probabilistic inference. Probabilistic programs can be
      systematically transformed to exploit vectorization opportunities in both modeling and inference.
      Our compiler uses lightweight effect handlers and JAX's tracing to partially evaluate inference
      logic at compile time, leaving only optimized array operations.</li>
    <li><strong>Formal Model.</strong> We develop $\lambda_{\text{GEN}}$, a calculus for probabilistic programming
      and programmable inference, on top of a core probabilistic array language. We define
      <code>vmap</code> as a source-to-source program transformation, prove its correctness, and show
      how it interacts with programmable inference interfaces.</li>
    <li><strong>Empirical Evaluation.</strong> We evaluate on benchmarks and case studies:
      <ul>
        <li><strong>Performance:</strong> GenJAX achieves near-handcoded JAX performance (100.1% relative time
          on Beta-Bernoulli), outperforming existing vectorized PPLs.</li>
        <li><strong>High-dimensional inference:</strong> We develop vectorized inference for probabilistic
          Game of Life inversion and robot localization with simulated LIDAR.</li>
      </ul>
    </li>
  </ol>
</section>

</div><!-- end container -->

<!-- Track Switcher -->
<section class="track-switcher-section" aria-labelledby="track-heading">
  <div class="container">
    <h2 id="track-heading">Explore the Paper</h2>
    <p class="track-description">Choose your path through the material. The <strong>Tutorial</strong> track offers progressive examples and intuitive explanations. The <strong>Theory</strong> track presents formal semantics, type systems, and proofs. <strong>All</strong> shows everything.</p>
    <div class="track-switcher" role="tablist" aria-label="Content tracks">
      <button class="track-btn tutorial-track active" id="btn-tutorial" 
              role="tab" aria-selected="true" aria-controls="track-tutorial" tabindex="0">
        <span class="track-icon">üìö</span>
        <span class="track-label">Tutorial</span>
        <span class="track-desc">Progressive examples &amp; intuitive explanations</span>
      </button>
      <button class="track-btn theory-track" id="btn-theory"
              role="tab" aria-selected="false" aria-controls="track-theory" tabindex="-1">
        <span class="track-icon">üî¨</span>
        <span class="track-label">Theory</span>
        <span class="track-desc">Formal model, type systems &amp; proofs</span>
      </button>
      <button class="track-btn all-track" id="btn-all"
              role="tab" aria-selected="false" aria-controls="track-all" tabindex="-1">
        <span class="track-icon">üìö‚Äçüî¨</span>
        <span class="track-label">All</span>
        <span class="track-desc">Show all content from both tracks</span>
      </button>
    </div>
  </div>
</section>

<div class="container">
<div class="main-content">

<!-- Content Area -->
<div class="content-area">

<!-- TUTORIAL TRACK CONTENT -->
<div id="track-tutorial" class="track-content active" role="tabpanel" aria-labelledby="btn-tutorial">

<section id="tutorial" aria-labelledby="tutorial-heading">
  <h2 id="tutorial-heading">Tutorial: Vectorized Probabilistic Programming</h2>
  
  <p>
    Probabilistic programming enables Bayesian inference over complex models, but scaling inference
    to large datasets and complex posteriors requires exploiting parallel hardware. This tutorial
    introduces GenJAX through a running example‚Äîpolynomial regression with outlier detection‚Äî
    showing how vectorization with <code>vmap</code> enables efficient inference on GPU.
  </p>
  
  <h3>Core Concepts</h3>
  
  <p>
    GenJAX is built around four key abstractions that work together to enable composable,
    vectorizable probabilistic computation:
  </p>
  
  <dl class="concept-list">
    <dt>Generative Functions</dt>
    <dd>
      Python functions decorated with <code>@gen</code> that define probabilistic computations.
      They represent probability distributions over both return values and internal random choices.
    </dd>
    
    <dt>Addresses</dt>
    <dd>
      String-valued names assigned to random choices via the syntax <code>dist @ "name"</code>.
      Addresses create a namespace for random variables, enabling trace manipulation and conditioning.
    </dd>
    
    <dt>Traces</dt>
    <dd>
      Structured records of all random choices made during execution of a generative function.
      Traces are the data lingua franca of programmable inference, enabling operations like
      scoring, updating, and projecting to subsets of variables.
    </dd>
    
    <dt>The Generative Function Interface (GFI)</dt>
    <dd>
      A standardized set of methods that every compiled generative function implements:
      <code>simulate</code> (run forward), <code>assess</code> (compute density),
      <code>update</code> (modify traces), and <code>project</code> (extract subsets).
    </dd>
  </dl>
  
  <div class="tutorial-step">
    <h3>Step 1: A Simple Polynomial Model</h3>
    
    <p>
      We begin with polynomial regression: given noisy datapoints $(x_i, y_i)$,
      we wish to infer the quadratic coefficients $(a, b, c)$ relating $x$ and $y$.
      Our model has three components:
    </p>
    
    <ul>
      <li><strong>Prior:</strong> A generative function <code>polynomial</code> that samples
        coefficients from standard normal distributions.</li>
      <li><strong>Likelihood:</strong> A generative function <code>point</code> that generates
        a noisy observation given a specific $x$ value and coefficients.</li>
      <li><strong>Dataset model:</strong> A generative function <code>npoint_curve</code> that
        generates an entire dataset by mapping <code>point</code> over a vector of $x$ values.</li>
    </ul>
    
    <div class="code-block">
      <div class="code-header">polynomial_regression.py</div>
      <pre><code>@gen
def polynomial():
    """Prior on quadratic coefficients."""
    a = normal(0.0, 1.0) @ "a"
    b = normal(0.0, 1.0) @ "b"  
    c = normal(0.0, 1.0) @ "c"
    return (a, b, c)

@gen
def point(x, coeffs):
    """Generate a single noisy observation."""
    a, b, c = coeffs
    y_mean = a * x**2 + b * x + c
    y = normal(y_mean, 0.3) @ "y"
    return y

@gen
def npoint_curve(xs):
    """Model entire dataset via vmap."""
    coeffs = polynomial()
    # vmap applies 'point' to each element of xs
    ys = vmap(point, in_axes=(0, None))(xs, coeffs)
    return ys</code></pre>
    </div>
    
    <p>
      The <code>vmap</code> transformation exploits <em>conditional independence</em>: given the
      coefficients, each datapoint is generated independently. This allows GenJAX to generate
      all $n$ points in parallel rather than sequentially.
    </p>
    
    <div class="info-box tip">
      <div class="info-box-title">Key Mechanism: Trace Vectorization</div>
      <p>
        When <code>vmap</code> transforms a generative function, it transforms the trace structure too:
        from <em>array-of-structs</em> (one trace object per execution) to <em>struct-of-arrays</em>
        (each address contains a vector of values). This memory layout enables efficient GPU computation.
      </p>
    </div>
    
    <div class="figure">
      <img src="static/images/curvefit_prior_multipoint_traces_density.png"
           alt="Prior samples from the polynomial model"
           loading="lazy" />
      <p class="fig-caption"><strong>Figure 2.</strong> Prior samples from the polynomial generative model.
      Each trace samples coefficients $(a, b, c)$ and generates noisy observations via the <code>vmap</code>ped
      point model. The traces show the variability in the prior before seeing any data.</p>
    </div>
  </div>
  
  <div class="tutorial-step">
    <h3>Step 2: Vectorized Importance Sampling</h3>
    
    <p>
      To perform inference, we use the GFI methods. <code>simulate</code> runs the program forward
      and returns a trace; <code>assess</code> computes the log-probability of a trace.
      These primitives compose to implement inference algorithms.
    </p>
    
    <p>
      Importance sampling is the simplest algorithm: sample many possible explanations (traces)
      from the prior, weight each by how well it explains the observed data (likelihood ratio),
      and use the weighted samples to approximate the posterior.
    </p>
    
    <div class="code-block">
      <div class="code-header">inference.py</div>
      <pre><code># Single-particle importance sampling
def importance_sample(model, obs, key):
    trace = model.simulate(key)
    weight = model.assess(trace, obs)
    return trace, weight

# Vectorize to N particles via vmap
vectorized_is = vmap(importance_sample, 
                     in_axes=(None, None, 0))

# Run with 10,000 particles ‚Äî executes in parallel
keys = split(key, 10000)
traces, weights = vectorized_is(model, obs, keys)</code></pre>
    </div>
    
    <p>
      By vectorizing both <code>simulate</code> and <code>assess</code>, we scale to thousands
      of particles that execute simultaneously on GPU. The vectorization is compositional:
      <code>vmap</code> of a GFI method is itself a GFI method operating on vectorized traces.
    </p>
    
    <div class="figure-row two-col">
      <div class="figure">
        <img src="static/images/curvefit_scaling_performance.png"
             alt="Scaling behavior of vectorized importance sampling"
             loading="lazy" />
        <p class="fig-caption"><strong>Figure 4a.</strong> Vectorized importance sampling exhibits
        near-constant wall-clock time on GPU as particle count increases, up to device memory limits.
        This enables scaling to $10^5$ particles in milliseconds.</p>
      </div>
      <div class="figure">
        <img src="static/images/curvefit_posterior_scaling_combined.png"
             alt="Posterior approximation at different particle counts"
             loading="lazy" />
        <p class="fig-caption"><strong>Figure 4b.</strong> Posterior approximations with increasing
        particle counts. Accuracy improves until convergence, demonstrating the statistical benefit
        of massive vectorization.</p>
      </div>
    </div>
  </div>
  
  <div class="tutorial-step">
    <h3>Step 3: Stochastic Branching for Outlier Detection</h3>
    
    <p>
      The simple model assumes all data follows the same noise distribution. Real data often
      violates this assumption. We enrich the model to handle outliers using
      <em>stochastic branching</em>.
    </p>
    
    <p>
      Each datapoint now has a latent "outlier flag" sampled from a Bernoulli distribution.
      If the flag is true, the observation comes from a broad uniform distribution (outlier);
      otherwise, it follows our precise polynomial model (inlier). The <code>Cond</code> combinator
      implements this branching while maintaining full vectorization.
    </p>
    
    <div class="code-block">
      <div class="code-header">outlier_model.py</div>
      <pre><code>@gen
def point_with_outlier(x, coeffs):
    """Point model with outlier detection."""
    a, b, c = coeffs
    
    # Latent outlier flag
    is_outlier = bernoulli(0.1) @ "outlier"
    
    # Branch based on the random flag
    y = Cond(is_outlier,
        # Outlier: broad uniform distribution
        lambda: uniform(-10, 10) @ "y",
        # Inlier: precise polynomial with noise
        lambda: normal(a*x**2 + b*x + c, 0.3) @ "y"
    )
    return y</code></pre>
    </div>
    
    <p>
      Stochastic branching maintains vectorization even when different elements take different
      branches. The <code>Cond</code> combinator evaluates both branches and selects elements
      based on the condition, enabling mixture models and regime-switching dynamics.
    </p>
    
    <div class="info-box warning">
      <div class="info-box-title">Inference Challenge</div>
      <p>
        The outlier model is more expressive but harder to fit. Importance sampling can identify
        likely outliers, but posterior uncertainty remains high even with $10^5$ particles‚Äîa form
        of underfitting caused by the added latent variables.
      </p>
    </div>
  </div>
  
  <div class="tutorial-step" data-step="4">
    <h3>Step 4: Programmable Inference with Gibbs + HMC</h3>
    
    <p>
      When generic inference fails, GenJAX enables custom algorithms tailored to the model structure.
      We design a hybrid algorithm combining two techniques:
    </p>
    
    <ul>
      <li><strong>Gibbs sampling</strong> for the discrete outlier flags, exploiting conditional
        independence to update all flags in parallel.</li>
      <li><strong>Hamiltonian Monte Carlo (HMC)</strong> for the continuous polynomial coefficients,
        using gradient information to explore the posterior efficiently.</li>
    </ul>
    
    <div class="code-block">
      <div class="code-header">gibbs_hmc.py</div>
      <pre><code>@gen
def gibbs_step(trace, key):
    """Vectorized Gibbs update for outlier flags."""
    # Extract subtrace containing outlier choices
    outlier_subtrace = trace.get_subtrace("ys")
    
    # For each data point, enumerate both possibilities (True/False)
    # and compute unnormalized posterior densities
    for flag in [True, False]:
        new_trace, weight = trace.update(
            key, {"outlier": flag}
        )
        log_probs[flag] = weight
    
    # Resample each flag from categorical over possibilities
    new_flags = categorical(softmax(log_probs)) @ "outliers"
    return updated_trace

# Hybrid kernel: Gibbs for discrete, HMC for continuous
inference_kernel = compose(gibbs_step, hmc_step)

# Run vectorized inference
inference_kernel = vmap(inference_kernel, in_axes=(0, 0))</code></pre>
    </div>
    
    <p>
      Because the outlier flags are conditionally independent given the curve parameters,
      we vectorize the Gibbs updates across all data points. The hybrid algorithm produces
      accurate posterior samples that correctly identify outliers while tightly fitting
      the polynomial trend to inlier data.
    </p>
    
    <div class="info-box tip">
      <div class="info-box-title">Key Insight: Three Levels of Vectorization</div>
      <p>
        GenJAX enables vectorization at three levels: <strong>(1)</strong> in the model
        (e.g., <code>vmap(point)</code> over datapoints), <strong>(2)</strong> in proposals
        (e.g., enumerative grids), and <strong>(3)</strong> across particles or chains
        (e.g., $10^4$ parallel MCMC chains). Every opportunity for parallelism is expressed
        with <code>vmap</code>, leading to highly efficient inference on GPU.
      </p>
    </div>
  </div>
  
  <!-- Tutorial Navigation -->
  <div class="tutorial-progress">
    <a href="#tutorial-step-1" class="tutorial-nav-btn" id="tut-prev">
      <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <polyline points="15 18 9 12 15 6"></polyline>
      </svg>
      <span>Previous</span>
    </a>
    <div class="tutorial-progress-dots">
      <div class="progress-dot completed" data-step="1"></div>
      <div class="progress-dot completed" data-step="2"></div>
      <div class="progress-dot completed" data-step="3"></div>
      <div class="progress-dot active" data-step="4"></div>
    </div>
    <a href="#theory" class="tutorial-nav-btn" onclick="switchTrack('theory'); return false;">
      <span>Theory Track</span>
      <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <polyline points="9 18 15 12 9 6"></polyline>
      </svg>
    </a>
  </div>
  
  <div class="info-box" style="margin-top: var(--space-xl);">
    <div class="info-box-title">Next Steps</div>
    <p>
      Continue to the <strong>Theory Track</strong> for the formal semantics of $\lambda_{\text{GEN}}$,
      including the denotational semantics, program transformations, and correctness proofs.
      Or explore the <a href="#evaluation">Evaluation</a> section for benchmark results
      and case studies.
    </p>
  </div>
</section>

</div><!-- end tutorial track -->

<!-- THEORY TRACK CONTENT -->
<div id="track-theory" class="track-content" role="tabpanel" aria-labelledby="btn-theory">

<section id="theory" aria-labelledby="theory-heading">
  <h2 id="theory-heading">Theory: $\lambda_{\text{GEN}}$ ‚Äî A Core Calculus for Probabilistic Programming</h2>
  
  <p>
    This section presents the formal foundations of GenJAX. We introduce $\lambda_{\text{GEN}}$,
    a simply-typed lambda calculus for probabilistic programming with programmable inference,
    define vectorization as a source-to-source transformation, and prove correctness.
    The formalism establishes the semantic properties that enable GenJAX's compositional
    vectorization of probabilistic computations.
  </p>
  
  <h3>The Core Calculus</h3>
  
  <p>
    $\lambda_{\text{GEN}}$ extends a standard array programming calculus in two main ways:
    a <strong>probability monad</strong> $\mathbb{P}$ for stochastic computations, and
    a <strong>graded monad</strong> $\mathbb{G}^\gamma$ of <em>generative functions</em>
    (traced probabilistic programs). The grading $\gamma$ tracks the address structure
    of random choices, enabling hierarchical traces and compositional inference.
  </p>
  
  <h4>Types</h4>
  
  <div class="math-block">
    $$\begin{aligned}
    \text{Base types } B &::= \mathbb{B} \mid \mathbb{R} \mid \mathbb{R}_{>0} \\
    \text{Batched types } T &::= B \mid T[n] \\
    \text{Ground types } \eta &::= 1 \mid T \mid \eta_1 \times \eta_2 
      \mid \{k_1 : \eta_1, \ldots, k_n : \eta_n\} \\
    \text{Types } \tau &::= \eta \mid \tau_1 \to \tau_2 \mid \tau_1 \times \tau_2 \\
      &\quad \mid \mathbb{D}\{\eta\} \mid \mathbb{P}\{\eta\} 
      \mid \mathbb{G}^\gamma\{\eta\}
    \end{aligned}$$
  </div>
  
  <p>
    The type $\mathbb{D}\{\eta\}$ represents density-carrying distributions over $\eta$.
    The graded monad $\mathbb{G}^\gamma\{\eta\}$ records addresses via the grading
    $\gamma = \{k_1 : \eta_1, \ldots, k_n : \eta_n\}$. We equip gradings with a monoid
    structure $\doubleplus$ for concatenation, with unit $\{\}$ (the empty record).
  </p>
  
  <h4>Terms and Typing Rules</h4>
  
  <div class="math-block">
    $$\begin{aligned}
    \text{Terms } t &::= () \mid c \mid p \mid x \mid (t_1, t_2) \mid \pi_i t \\
      &\quad \mid t[k] \mid \{k_1 : t_1, \ldots, k_n : t_n\} \\
      &\quad \mid t_1~t_2 \mid \lambda x.t \mid \textbf{let}~x = t_1~\textbf{in}~t_2 \\
      &\quad \mid \textbf{select}(t_1, t_2, t_3) \mid \textbf{trace}_k\{t\} \\
      &\quad \mid \textbf{return}_\mathbb{G}\{t\} \mid \textbf{return}_\mathbb{P}\{t\} \\
      &\quad \mid \textbf{do}_\mathbb{G}\{m\} \mid \textbf{do}_\mathbb{P}\{m\} \mid \textbf{sample}~t \\
    \text{Monadic } m &::= t \mid x \leftarrow t; m
    \end{aligned}$$
  </div>
  
  <table class="comparison-table">
    <thead>
      <tr>
        <th>Construct</th>
        <th>Syntax</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Sample</td>
        <td><code>sample t</code></td>
        <td>$\mathbb{P}\{\eta\}$</td>
        <td>Sample from distribution $t : \mathbb{D}\{\eta\}$</td>
      </tr>
      <tr>
        <td>Return (prob.)</td>
        <td><code>return<sub>‚Ñô</sub> t</code></td>
        <td>$\mathbb{P}\{\eta\}$</td>
        <td>Lift deterministic $t : \eta$ to probability monad</td>
      </tr>
      <tr>
        <td>Trace</td>
        <td><code>trace<sub>k</sub> t</code></td>
        <td>$\mathbb{G}^{\{k:\gamma\}}\{\eta\}$</td>
        <td>Name generative function at address $k$</td>
      </tr>
      <tr>
        <td>Bind (gen.)</td>
        <td><code>do<sub>ùîæ</sub>{x ‚Üê t; m}</code></td>
        <td>$\mathbb{G}^{\gamma \doubleplus \gamma'}\{\eta'\}$</td>
        <td>Sequence generative functions (disjoint addresses)</td>
      </tr>
      <tr>
        <td>Select</td>
        <td><code>select(c, t‚ÇÅ, t‚ÇÇ)</code></td>
        <td>$T^s$</td>
        <td>Batched conditional selection</td>
      </tr>
    </tbody>
  </table>
  
  <h3>Denotational Semantics</h3>
  
  <p>
    We give a denotational semantics for $\lambda_{\text{GEN}}$ using <strong>quasi-Borel spaces</strong> (QBS),
    a convenient category for higher-order probabilistic programming. QBS supports function spaces,
    products, and a probability monad while properly handling measure-theoretic concerns.
  </p>
  
  <div class="math-block">
    $$\begin{aligned}
    \sem{\mathbb{B}} &= \mathbb{B} \quad \sem{\mathbb{R}} = \mathbb{R} \quad
    \sem{T[n]} = \sem{T}^n \quad \sem{1} = 1 \\
    \sem{\eta_1 \times \eta_2} &= \sem{\eta_1} \times \sem{\eta_2} \\
    \sem{\{k_1 : \eta_1, \ldots, k_n : \eta_n\}} &= \sem{\eta_1} \times \cdots \times \sem{\eta_n} \\
    \sem{\tau_1 \to \tau_2} &= [\sem{\tau_1}, \sem{\tau_2}] \quad \text{(QBS function space)} \\
    \sem{\mathbb{P}\{\eta\}} &= \mathcal{P}\sem{\eta} \quad \text{(probability monad)} \\
    \sem{\mathbb{D}\{\eta\}} &= \mathcal{P}_{\ll}\sem{\eta} \quad \text{(absolutely continuous measures)} \\
    \sem{\mathbb{G}^\gamma\{\eta\}} &= \mathcal{P}_{\ll}\sem{\gamma} \times [\sem{\gamma}, \sem{\eta}]
    \end{aligned}$$
  </div>
  
  <p>
    A generative function denotes a pair: a measure on traces (absolutely continuous w.r.t.
    the stock measure), and a return value function $\sem{\gamma} \to \sem{\eta}$ that
    computes the program's output given values for all random choices.
  </p>
  
  <h3>Programmable Inference Transformations</h3>
  
  <p>
    Generative functions support two fundamental operations implemented as source-to-source
    program transformations:
  </p>
  
  <div class="math-block">
    $$\begin{aligned}
    \text{simulate} : \mathbb{G}^\gamma\{\eta\} &\to \mathbb{P}\{(\gamma \times \eta \times \mathbb{R})\} \\
    \text{assess} : \mathbb{G}^\gamma\{\eta\} &\to \gamma \to (\eta \times \mathbb{R})
    \end{aligned}$$
  </div>
  
  <p>
    <strong>Simulate</strong> runs the program forward, collecting a trace, return value, and joint density.
    <strong>Assess</strong> computes the density of a given trace by replaying the program with
    constrained values. These transformations act homomorphically on types and terms.
  </p>
  
  <div class="theorem-box">
    <div class="theorem-title">Proposition (Correctness of Simulate and Assess)</div>
    <p>
      Let $\vdash t : \mathbb{G}^\gamma\{\eta\}$ with denotation $(\mu, f) = \sem{t}$
      and let $\nu$ be the stock measure on $\gamma$. Then:
    </p>
    <ul>
      <li>$\sem{\text{simulate}\{t\}} = \langle \text{id}, f, \frac{d\mu}{d\nu}\rangle_*\mu$ ‚Äî 
        simulate faithfully generates traces from $\mu$ with their densities.</li>
      <li>$\sem{\text{assess}\{t\}} = \langle f, \frac{d\mu}{d\nu}\rangle$ ‚Äî 
        assess faithfully computes return values and densities at given traces.</li>
    </ul>
  </div>
  
  <h3>Vectorization as Program Transformation</h3>
  
  <p>
    We define $\text{vmap}_n$ as a type-directed program transformation that takes a term
    of type $\tau$ and produces a vectorized term of type $\tau[n]$, representing $n$
    parallel executions.
  </p>
  
  <h4>Type-Level Vectorization</h4>
  
  <div class="math-block">
    $$\begin{aligned}
    (\mathbb{D}\{\eta\})[n] &::= \mathbb{D}\{\eta[n]\} \\
    (\mathbb{G}^\gamma\{\eta\})[n] &::= \mathbb{G}^{\gamma[n]}\{\eta[n]\} \\
    (\mathbb{P}\{\eta\})[n] &::= \mathbb{P}\{\eta[n]\} \\
    \{k_1 : \eta_1, \ldots\}[n] &::= \{k_1 : \eta_1[n], \ldots\} \\
    (\tau_1 \to \tau_2)[n] &::= \tau_1[n] \to \tau_2[n]
    \end{aligned}$$
  </div>
  
  <p>
    The transformation performs an automatic <em>array-of-struct</em> to <em>struct-of-array</em>
    conversion on traces. On primitives, $\text{vmap}$ extends the batching shape.
    On other terms, it acts homomorphically.
  </p>
  
  <h4>Logical Relations for Correctness</h4>
  
  <p>
    We define logical relations $\mathcal{R}_\tau \subseteq \sem{\tau}^n \times \sem{\tau[n]}$
    encoding when a vectorized value correctly represents $n$ scalar values:
  </p>
  
  <div class="math-block">
    $$\begin{aligned}
    \mathcal{R}_B &= \{((x_1,\ldots,x_n), x) \mid x = (x_1,\ldots,x_n)\} \\
    \mathcal{R}_{\tau_1 \to \tau_2} &= \{((f_1,\ldots,f_n), g) \mid 
      \forall ((x_i), y) \in \mathcal{R}_{\tau_1},\ ((f_i(x_i)), g(y)) \in \mathcal{R}_{\tau_2}\} \\
    \mathcal{R}_{\mathbb{G}^\gamma\{\eta\}} &= \{((\mu_i, f_i), (\nu, g)) \mid
      ((\mu_i), \nu) \in \mathcal{R}_{\mathbb{P}\{\gamma\}},\ ((f_i), g) \in \mathcal{R}_{\gamma \to \eta}\}
    \end{aligned}$$
  </div>
  
  <div class="theorem-box">
    <div class="theorem-title">Theorem (Correctness of Vectorization)</div>
    <p>
      For closed $\vdash t : T \to \tau$ and input $v$ of type $T[n]$:
    </p>
    <ul>
      <li><strong>Deterministic:</strong> If $\tau = \eta$, then
        $\sem{\text{vmap}_n\{t\}}(v) = \text{zip}_\eta(\sem{t}(v[1]), \ldots, \sem{t}(v[n]))$.</li>
      <li><strong>Probabilistic:</strong> If $\tau = \mathbb{P}\{\eta\}$, then
        $\sem{\text{vmap}_n\{t\}}(v) = \text{zip}_{\eta*}\bigotimes_{i=1}^n \sem{t}(v[i])$.
        Vectorization produces independent samples.</li>
      <li><strong>Generative:</strong> If $\tau = \mathbb{G}^\gamma\{\eta\}$, both trace
        distributions and return value maps vectorize correctly.</li>
    </ul>
  </div>
  
  <div class="theorem-box">
    <div class="theorem-title">Corollary (Commutativity)</div>
    <p>
      Vectorization commutes with programmable inference operations:
    </p>
    <ul>
      <li>$\sem{\text{simulate}\{\text{vmap}_n\{t\}\}} = 
        \langle \text{id}, \text{id}, v \mapsto \prod_i v[i]\rangle_*\sem{\text{vmap}_n\{\text{simulate}\{t\}\}}$</li>
      <li>$\sem{\text{assess}\{\text{vmap}_n\{t\}\}}(u) = 
        \text{let}~(a, b) = \sem{\text{vmap}_n\{\text{assess}\{t\}\}}(u)~\text{in}~(a, \prod_i b[i])$</li>
    </ul>
  </div>
  
  <h3>Stochastic Branching</h3>
  
  <p>
    To support mixture models and regime-switching dynamics, we extend the calculus with
    <strong>stochastic branching</strong>. The $\textbf{cond}(t_1, t_2, t_3, t_4)$ construct
    selects between two generative function branches based on random Boolean values.
  </p>
  
  <div class="math-block">
    $$\frac{\Gamma \vdash t_1 : \mathbb{B}^s \quad \vdash t_2 : \eta_1 \to \mathbb{G}^\gamma\{\eta_2\}
      \quad \vdash t_3 : \eta_1 \to \mathbb{G}^\gamma\{\eta_2\} \quad \Gamma \vdash t_4 : \eta_1[s]}
      {\Gamma \vdash \textbf{cond}(t_1, t_2, t_3, t_4) : \mathbb{G}^{\gamma[s]}\{\eta_2[s]\}}$$
  </div>
  
  <p>
    The $\text{simulate}$ and $\text{assess}$ transformations handle branching by evaluating
    both branches and selecting elements via $\textbf{select}$. Our implementation pads traces
    with sentinel values to support heterogeneous gradings.
  </p>
  
  <h3>Compiler Architecture</h3>
  
  <p>
    The GenJAX compiler translates Python with <code>@gen</code> decorators to JAX-compatible
    code through several stages:
  </p>
  
  <ol>
    <li><strong>Tracing:</strong> Python control flow is staged via JAX tracing, capturing
      the computational graph.</li>
    <li><strong>Effect Handling:</strong> Random choices are reified as effect operations,
      routed to struct-of-array trace data structures.</li>
    <li><strong>Partial Evaluation:</strong> Inference logic is evaluated at compile time
      where possible, leaving only optimized array operations.</li>
    <li><strong>Code Generation:</strong> JAX primitives with efficient memory layout,
      supporting automatic differentiation and TPU/GPU compilation.</li>
  </ol>
  
  <p>
    The formal results above guarantee that these compiler transformations preserve semantics
    while enabling efficient vectorized execution on modern accelerators.
  </p>
</section>

</div><!-- end theory track -->

</div><!-- end content-area -->

<!-- Sidebar Navigation -->
<aside class="sidebar" aria-label="Table of contents">
  <div class="toc-title">On this page</div>
  <nav class="toc-nav" aria-label="Page sections">
    <ul>
      <li><a href="#abstract" class="toc-h2">Abstract</a></li>
      <li><a href="#overview" class="toc-h2">Overview</a></li>
      <li><a href="#tutorial" class="toc-h2">Tutorial Track</a>
        <ul>
          <li><a href="#tutorial" class="toc-h3">Core Concepts</a></li>
          <li><a href="#tutorial" class="toc-h3">Step 1: Model</a></li>
          <li><a href="#tutorial" class="toc-h3">Step 2: Inference</a></li>
          <li><a href="#tutorial" class="toc-h3">Step 3: Branching</a></li>
          <li><a href="#tutorial" class="toc-h3">Step 4: Gibbs + HMC</a></li>
        </ul>
      </li>
      <li><a href="#theory" class="toc-h2">Theory Track</a>
        <ul>
          <li><a href="#theory" class="toc-h3">Core Calculus</a></li>
          <li><a href="#theory" class="toc-h3">Semantics</a></li>
          <li><a href="#theory" class="toc-h3">Vectorization</a></li>
          <li><a href="#theory" class="toc-h3">Branching</a></li>
        </ul>
      </li>
      <li><a href="#evaluation" class="toc-h2">Evaluation</a>
        <ul>
          <li><a href="#evaluation" class="toc-h3">Performance</a></li>
          <li><a href="#evaluation" class="toc-h3">Game of Life</a></li>
          <li><a href="#evaluation" class="toc-h3">Localization</a></li>
        </ul>
      </li>
      <li><a href="#artifact" class="toc-h2">Artifact</a></li>
      <li><a href="#citation" class="toc-h2">Citation</a></li>
    </ul>
  </nav>
  
  <div class="track-widget" style="margin-top: var(--space-2xl); padding-top: var(--space-xl); border-top: 1px solid var(--color-border);">
    <div class="toc-title">View Mode</div>
    <div class="track-widget-buttons">
      <a href="#tutorial" class="track-widget-btn" onclick="if(window.GenJAX) { window.GenJAX.switchTrack('tutorial'); return false; }">
        <span class="track-dot" style="background: var(--color-tutorial);"></span> Tutorial
      </a>
      <a href="#theory" class="track-widget-btn" onclick="if(window.GenJAX) { window.GenJAX.switchTrack('theory'); return false; }">
        <span class="track-dot" style="background: var(--color-theory);"></span> Theory
      </a>
      <a href="#all" class="track-widget-btn" onclick="if(window.GenJAX) { window.GenJAX.switchTrack('all'); return false; }">
        <span class="track-dot" style="background: var(--color-all);"></span> All
      </a>
    </div>
  </div>
</aside>

</div><!-- end main-content -->

<!-- EVALUATION (Common section) -->
<section id="evaluation" aria-labelledby="evaluation-heading">
  <h2 id="evaluation-heading">Evaluation <a href="#evaluation" class="section-anchor" aria-label="Link to Evaluation">#</a></h2>
  
  <p>
    We evaluate GenJAX on benchmarks and case studies designed to assess two key criteria:
    <strong>performance</strong> (how does our compiler compare to leading PPLs and hand-optimized code?)
    and <strong>inference quality</strong> (can we construct effective inference for high-dimensional problems?).
    Our results demonstrate that GenJAX achieves near-zero overhead compared to hand-coded JAX while
    enabling sophisticated inference algorithms on challenging problems.
  </p>
  
  <h3>Performance Survey</h3>
  
  <p>
    We compare GenJAX against NumPyro, Pyro, TensorFlow Probability, hand-coded JAX, PyTorch, and Gen.jl
    on two fundamental inference paradigms: importance sampling (embarrassingly parallel) and 
    Hamiltonian Monte Carlo (iterative, gradient-based).
  </p>
  
  <h4>Beta-Bernoulli Model</h4>
  
  <p>
    The Beta-Bernoulli model infers the bias of a coin from observed flips, using a Beta(1,1) prior 
    and Bernoulli likelihood. We observe 50 flips and construct a posterior approximation using 
    importance sampling.
  </p>
  
  <div class="figure">
    <img src="static/images/combined_3x2_obs50_samples2000.png"
         alt="Beta-Bernoulli model: posterior accuracy and timing comparison"
         loading="lazy" />
    <p class="fig-caption"><strong>Figure 6.</strong> Beta-Bernoulli model: all frameworks accurately recover 
    the true posterior. GenJAX achieves near-identical performance to hand-coded JAX (100.1% relative time).</p>
  </div>
  
  <h4>Scaling Analysis</h4>
  
  <p>
    On the polynomial regression problem from Section 2, we examine how inference time scales with 
    problem size. Importance sampling exhibits near-constant wall-clock time as particle count increases 
    (while GPU memory permits), validating the effectiveness of vectorization. HMC shows linear scaling 
    in chain length, as expected for iterative algorithms.
  </p>
  
  <div class="figure-row two-col">
    <div class="figure">
      <img src="static/images/benchmark_timings_is_all_frameworks.png"
           alt="Importance sampling timing comparison"
           loading="lazy" />
      <p class="fig-caption"><strong>Figure 7a.</strong> Importance sampling: vectorized frameworks show 
      near-constant scaling while GPU is not saturated. GenJAX matches hand-coded JAX.</p>
    </div>
    <div class="figure">
      <img src="static/images/benchmark_timings_hmc_all_frameworks.png"
           alt="HMC timing comparison"
           loading="lazy" />
      <p class="fig-caption"><strong>Figure 7b.</strong> Hamiltonian Monte Carlo: linear scaling in chain length. 
      GenJAX consistently matches hand-coded JAX performance.</p>
    </div>
  </div>
  
  <div class="info-box tip">
    <div class="info-box-title">Key Result: Zero-Cost Abstractions</div>
    <p>
      GenJAX achieves <strong>100.1% relative time</strong> compared to hand-coded JAX on the Beta-Bernoulli model.
      Our compiler eliminates abstraction overhead through partial evaluation at compile time, 
      leaving only optimized array operations that execute at native JAX speed.
    </p>
  </div>
  
  <h3>Case Study: Probabilistic Game of Life Inversion</h3>
  
  <p>
    Game of Life (GoL) inversion is the problem of inverting Conway's Game of Life dynamics: given a final state,
    what is a possible previous state that evolves to it? Brute-force search is intractable
    ($2^{N \times N}$ possible states for an $N \times N$ board).
  </p>
  
  <p>
    We introduce probabilistic noise into the dynamics: from an initial state, we evolve forward using 
    deterministic rules, then sample with Bernoulli noise around the true value of each cell. This 
    creates a probabilistic inversion problem amenable to approximate inference.
  </p>
  
  <h4>Vectorized Gibbs Sampling for GoL</h4>
  
  <p>
    Each cell's value is conditionally independent from non-neighboring cells given its eight neighbors.
    We partition cells into conditionally independent groups (a 2-coloring of the grid) and perform 
    parallel Gibbs updates ‚Äî an instance of <strong>chromatic Gibbs sampling</strong>.
    The vectorized implementation achieves up to 90% inversion accuracy on 1024√ó1024 grids in seconds.
  </p>
  
  <div class="figure-row two-col">
    <div class="figure">
      <img src="static/images/gol_integrated_showcase_wizards_1024.png"
           alt="Game of Life inversion showcase"
           loading="lazy" />
      <p class="fig-caption"><strong>Figure 9a.</strong> Approximate inversion of Game of Life dynamics 
      using vectorized Gibbs sampling on a 1024√ó1024 board (wizard book pattern).</p>
    </div>
    <div class="figure">
      <img src="static/images/gol_gibbs_timing_bar_plot.png"
           alt="Game of Life Gibbs sampling throughput"
           loading="lazy" />
      <p class="fig-caption"><strong>Figure 9b.</strong> Gibbs sampling throughput across grid sizes, 
      comparing CPU and GPU execution. Vectorization provides massive speedups on GPU.</p>
    </div>
  </div>
  
  <h3>Case Study: Robot Localization with Sequential Monte Carlo</h3>
  
  <p>
    Simultaneous Localization and Mapping (SLAM) is a fundamental robotics problem: constructing a 
    representation of the environment and the robot's position within it. When the map is given, 
    this reduces to the <strong>localization problem</strong>.
  </p>
  
  <p>
    We model a robot maneuvering through a known 2D space, receiving LIDAR distance measurements.
    The goal is to maintain a probabilistic belief over the robot's position. Using GenJAX's 
    programmable inference abstractions, we develop three SMC algorithms:
  </p>
  
  <table class="comparison-table">
    <thead>
      <tr>
        <th>Algorithm</th>
        <th>Proposal</th>
        <th>Key Feature</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Bootstrap Filter</td>
        <td>Model prior</td>
        <td>Baseline: particles sampled from dynamics</td>
      </tr>
      <tr>
        <td>SMC + HMC</td>
        <td>Prior + HMC rejuvenation</td>
        <td>Gradient-based moves after resampling</td>
      </tr>
      <tr>
        <td>SMC + Locally Optimal</td>
        <td>Vectorized grid enumeration</td>
        <td>Evaluates grid of positions, selects ML point</td>
      </tr>
    </tbody>
  </table>
  
  <p>
    The locally optimal proposal adds another layer of vectorization <em>within</em> the custom proposal:
    it enumerates a position grid, evaluates each against the observation likelihood (fully vectorized),
    selects the maximum likelihood point, and samples around it. Combined with vectorization across 
    particles and in the LIDAR measurement model, this yields a highly efficient algorithm that 
    tracks the robot's location in milliseconds.
  </p>
  
  <div class="figure-row two-col">
    <div class="figure">
      <img src="static/images/localization_r8_p200_basic_localization_problem_1x4_explanation.png"
           alt="Robot localization problem"
           loading="lazy" />
      <p class="fig-caption"><strong>Figure 10a.</strong> Localization problem: robot with 8-ray LIDAR 
      navigates a 2D environment. Goal is to infer position from noisy distance measurements.</p>
    </div>
    <div class="figure">
      <img src="static/images/localization_r8_p200_basic_comprehensive_4panel_smc_methods_analysis.png"
           alt="SMC methods comparison"
           loading="lazy" />
      <p class="fig-caption"><strong>Figure 10b.</strong> SMC methods comparison. The locally optimal proposal 
      with vectorized grid enumeration achieves the best accuracy-efficiency tradeoff.</p>
    </div>
  </div>
  
  <h3>Summary</h3>
  
  <p>
    Our evaluation demonstrates that GenJAX successfully unifies vectorization and programmable inference:
  </p>
  
  <ul>
    <li><strong>Performance:</strong> Zero-cost abstractions match hand-coded JAX; vectorized frameworks 
      outperform non-vectorized alternatives by orders of magnitude on GPU.</li>
    <li><strong>Expressivity:</strong> The <code>vmap</code> idiom naturally composes across model, proposal, 
      and particle dimensions, enabling sophisticated algorithms like chromatic Gibbs and locally optimal SMC.</li>
    <li><strong>Scalability:</strong> Problems with millions of latent variables (1024√ó1024 grids) can be 
      solved efficiently using GPU-accelerated vectorized inference.</li>
  </ul>
</section>

<!-- Artifact -->
<section id="artifact" aria-labelledby="artifact-heading">
  <h2 id="artifact-heading">Artifact & Reproducibility</h2>
  
  <p>
    The GenJAX artifact is permanently archived on <a href="https://doi.org/10.5281/zenodo.17342547">Zenodo</a> (DOI: 10.5281/zenodo.17342547).
    All experiments in the paper are reproducible from this artifact, which includes:
  </p>
  
  <div class="artifact-grid">
    <div class="artifact-card">
      <div class="artifact-icon">‚ö°</div>
      <h4>Compiler & Runtime</h4>
      <p>Complete GenJAX implementation with JAX integration, effect handlers, and GPU/TPU support.</p>
    </div>
    <div class="artifact-card">
      <div class="artifact-icon">üìä</div>
      <h4>Benchmarks</h4>
      <p>Performance comparison scripts for all frameworks (NumPyro, Pyro, TFP, Gen.jl) and models.</p>
    </div>
    <div class="artifact-card">
      <div class="artifact-icon">üî¨</div>
      <h4>Case Studies</h4>
      <p>Full implementations: Game of Life inversion, robot localization, polynomial regression.</p>
    </div>
    <div class="artifact-card">
      <div class="artifact-icon">üìì</div>
      <h4>Notebooks</h4>
      <p>Jupyter notebooks reproducing every figure and result from the paper.</p>
    </div>
  </div>
  
  <h3>Quick Start</h3>
  
  <div class="code-block">
    <div class="code-header">Installation</div>
    <pre><code># Install from PyPI (recommended)
pip install genjax

# Or install from source for development
git clone https://github.com/femtomc/genjax.git
cd genjax
pip install -e ".[dev]"

# Verify installation
python -c "import genjax; print(genjax.__version__)"</code></pre>
  </div>
  
  <div class="code-block">
    <div class="code-header">Your First GenJAX Program</div>
    <pre><code>import jax
import genjax
from genjax import gen, normal, vmap

@gen
def model(x):
    slope = normal(0.0, 1.0) @ "slope"
    intercept = normal(0.0, 1.0) @ "intercept"
    y = normal(slope * x + intercept, 0.1) @ "y"
    return y

# Create a vectorized version for batch inference
batched_model = vmap(model, in_axes=(0,))

# Run with JAX random key
key = jax.random.key(0)
xs = jax.numpy.linspace(-3, 3, 100)
traces = jax.jit(batched_model.simulate)(key, (xs,))</code></pre>
  </div>
  
  <h3>Hardware Requirements</h3>
  
  <table class="comparison-table">
    <thead>
      <tr>
        <th>Component</th>
        <th>Minimum</th>
        <th>Recommended</th>
        <th>Used in Paper</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>GPU</td>
        <td>Any CUDA-capable</td>
        <td>NVIDIA A100 / H100</td>
        <td>NVIDIA A100 40GB</td>
      </tr>
      <tr>
        <td>RAM</td>
        <td>8 GB</td>
        <td>32 GB</td>
        <td>40 GB (GPU memory)</td>
      </tr>
      <tr>
        <td>Python</td>
        <td>3.10</td>
        <td>3.11+</td>
        <td>3.11</td>
      </tr>
      <tr>
        <td>JAX</td>
        <td>0.4.20</td>
        <td>0.4.30+</td>
        <td>0.4.30</td>
      </tr>
    </tbody>
  </table>
  
  <div class="info-box tip">
    <div class="info-box-title">Reproducing Paper Results</div>
    <p>
      All benchmarks and figures can be reproduced via the provided notebooks.
      The artifact includes a <code>reproduce.sh</code> script that runs all experiments
      and generates comparison plots. Expected runtime: ~2 hours on A100 GPU.
    </p>
  </div>
</section>

<!-- Citation -->
<section id="citation" aria-labelledby="citation-heading">
  <h2 id="citation-heading">Citation</h2>
  
  <div class="bibtex-container">
    <button class="copy-btn" onclick="copyBibtex()" aria-label="Copy citation to clipboard">Copy</button>
    <pre class="bibtex-block" id="bibtex"><code>@article{becker2026genjax,
  title     = {Probabilistic Programming with Vectorized Programmable Inference},
  author    = {Becker, McCoy R. and Huot, Mathieu and Matheos, George and
               Wang, Xiaoyan and Chung, Karen and Smith, Colin and
               Ritchie, Sam and Saurous, Rif A. and Lew, Alexander K. and
               Rinard, Martin C. and Mansinghka, Vikash K.},
  journal   = {Proceedings of the ACM on Programming Languages},
  volume    = {10},
  number    = {POPL},
  articleno = {87},
  year      = {2026},
  publisher = {ACM},
  doi       = {10.1145/3776729},
}</code></pre>
  </div>
</section>

</div><!-- end container -->
</main>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <p>
      GenJAX is developed at the
      <a href="https://probcomp.csail.mit.edu/">MIT Probabilistic Computing Project</a>.
    </p>
    <p>
      ¬© 2026 The Authors. Published in PACMPL.
    </p>
  </div>
</footer>

<!-- Inline script for functions that need to be global for onclick handlers -->
<script>
  // Copy BibTeX function (used by onclick handler)
  function copyBibtex() {
    const bibtex = document.getElementById('bibtex').textContent;
    navigator.clipboard.writeText(bibtex).then(() => {
      const btn = document.querySelector('.copy-btn');
      const originalText = btn.textContent;
      btn.textContent = 'Copied!';
      setTimeout(() => btn.textContent = originalText, 2000);
    });
  }
</script>

</body>
</html>
