<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="GenJAX — a language and compiler for vectorized programmable probabilistic inference, built on JAX. POPL 2026." />
  <meta name="theme-color" content="#fdfcfa" />
  <meta name="color-scheme" content="light" />
  <title>Probabilistic Programming with Vectorized Programmable Inference</title>
  <link rel="stylesheet" href="static/css/style.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" />
  <link rel="icon" type="image/svg+xml" href="static/images/logo.svg" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:wght@400;500&family=Source+Sans+3:wght@400;500;600;700&family=Source+Serif+4:wght@400;600&display=swap" rel="stylesheet">
  <script>
    window.MathJax = {
      tex: { 
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        packages: { '[+]': ['textmacros'] },
        macros: {
          gen: "\\lambda_{\\text{GEN}}",
          vmap: "\\text{vmap}",
          simulate: "\\text{simulate}",
          assess: "\\text{assess}",
          sem: ["\\left[\\!\\left[ #1 \\right]\\!\\right]", 1],
          doubleplus: "\\mathbin{+\\!+}"
        }
      },
      svg: { fontCache: 'global' },
      startup: {
        typeset: false,
        ready: () => {
          MathJax.startup.defaultReady();
          document.dispatchEvent(new Event('mathjax:ready'));
        }
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js" defer></script>
  <script src="static/js/site.js" defer></script>
  <noscript>
    <style>
      .track-content { display: block !important; }
      .track-switcher { display: none; }
      .track-widget { display: none; }
      body::before {
        content: 'Note: JavaScript is disabled. Showing all content (Tutorial and Theory tracks).';
        display: block;
        background: var(--color-theory-bg);
        color: var(--color-theory);
        padding: var(--space-md) var(--space-xl);
        text-align: center;
        font-family: var(--font-sans);
        font-size: 0.9rem;
        border-bottom: 1px solid var(--color-border);
      }
    </style>
  </noscript>
</head>
<body>

<a href="#main-content" class="skip-link">Skip to main content</a>

<!-- Navigation -->
<nav class="site-nav" role="navigation" aria-label="Main navigation">
  <div class="container nav-content">
    <div class="nav-brand"><a href="#">GenJAX</a></div>
    <div class="nav-links">
      <a href="#introduction">Introduction</a>
      <a href="#tutorial">Tutorial</a>
      <a href="#theory">Theory</a>
      <a href="#evaluation">Evaluation</a>
      <a href="#conclusion">Conclusion</a>
      <a href="#artifact">Artifact</a>
      <a href="#citation">Citation</a>
    </div>
  </div>
</nav>

<!-- Header -->
<header class="paper-header">
  <div class="container">
    <img src="static/images/logo.png" alt="GenJAX" class="paper-logo" />
    <h1>Probabilistic Programming with<br>Vectorized Programmable Inference</h1>
    <p class="venue">POPL 2026 · Proceedings of the ACM on Programming Languages (PACMPL), Vol. 10, No. POPL, Article 87</p>
  </div>
</header>

<!-- Authors -->
<section class="authors-section" aria-label="Authors">
  <div class="container">
    <div class="authors">
      <span class="author-name">McCoy R. Becker*</span>,
      <span class="author-name">Mathieu Huot*</span>,
      <span class="author-name">George Matheos</span>,
      <span class="author-name">Xiaoyan Wang</span>,
      <span class="author-name">Karen Chung</span>,
      <span class="author-name">Colin Smith</span>,
      <span class="author-name">Sam Ritchie</span>,
      <span class="author-name">Rif A. Saurous</span>,
      <span class="author-name">Alexander K. Lew</span>,
      <span class="author-name">Martin C. Rinard</span>,
      <span class="author-name">Vikash K. Mansinghka</span>
      <span class="equal-contrib">* Equal contribution</span>
    </div>
    <div class="affiliations">
      Massachusetts Institute of Technology · Google · Yale University
    </div>
  </div>
</section>

<main id="main-content">
<div class="container">

<!-- Quick Links -->
<section id="links" aria-label="Quick links">
  <div class="links-grid">
    <a href="static/paper.pdf" class="link-btn">
      <svg viewBox="0 0 24 24" fill="currentColor" aria-hidden="true"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8l-6-6zm-1 2l5 5h-5V4zM6 20V4h5v7h7v9H6z"/></svg>
      Paper (PDF)
    </a>
    <a href="https://dl.acm.org/doi/10.1145/3776729" class="link-btn">
      <svg viewBox="0 0 24 24" fill="currentColor" aria-hidden="true"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-1 17.93c-3.95-.49-7-3.85-7-7.93 0-.62.08-1.21.21-1.79L9 15v1a2 2 0 0 0 2 2v1.93zm6.9-2.54A1.99 1.99 0 0 0 16 16h-1v-3a1 1 0 0 0-1-1H8v-2h2a1 1 0 0 0 1-1V7h2a2 2 0 0 0 2-2v-.41A7.997 7.997 0 0 1 20 12c0 2.08-.8 3.97-2.1 5.39z"/></svg>
      ACM Digital Library
    </a>
    <a href="https://github.com/probcomp/genjax" class="link-btn">
      <svg viewBox="0 0 16 16" fill="currentColor" aria-hidden="true"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/></svg>
      Source Code
    </a>
    <a href="https://doi.org/10.5281/zenodo.17594132" class="link-btn">
      <svg viewBox="0 0 24 24" fill="currentColor" aria-hidden="true"><path d="M12 2L2 7v10l10 5 10-5V7L12 2zm0 2.18l7 3.5v7.64l-7 3.5-7-3.5V7.68l7-3.5z"/></svg>
      Zenodo Artifact
    </a>
  </div>
</section>

<!-- Abstract -->
<section id="abstract" aria-labelledby="abstract-heading">
  <h2 id="abstract-heading">Abstract <a href="#abstract" class="section-anchor" aria-label="Link to Abstract">#</a></h2>
  <div class="abstract">
    <p>
      We present GenJAX, a new language and compiler for vectorized programmable probabilistic inference.
      GenJAX integrates the vectorizing map (<code>vmap</code>) operation from array programming frameworks such as JAX
      into the programmable inference paradigm, enabling compositional vectorization of features such as probabilistic program traces,
      stochastic branching (for expressing mixture models), and programmable inference interfaces for writing custom probabilistic inference algorithms.
      We formalize vectorization as a source-to-source program transformation on a core calculus for probabilistic programming ($\gen$), and
      prove that it correctly vectorizes both modeling and inference operations.
      We have implemented our approach in <a href="https://github.com/probcomp/genjax">the GenJAX language and compiler</a>,
      and have empirically evaluated this implementation on several benchmarks and case studies. Our results show that our implementation
      supports a wide and expressive set of programmable inference patterns and delivers performance comparable to hand-optimized JAX code.
    </p>
  </div>
</section>

<!-- Introduction Section (Common to both tracks) -->
<section id="introduction" data-nav-section aria-labelledby="introduction-heading">
  <h2 id="introduction-heading">Introduction <a href="#introduction" class="section-anchor" aria-label="Link to Introduction">#</a></h2>

  <p>
    In recent years, probabilistic programming has demonstrated remarkable effectiveness in a range of application domains, including
    3D perception and scene understanding, probabilistic robotics, automated data cleaning and analysis, particle physics, time series
    structure discovery, test-time control of large language models, and cognitive modeling of theory of mind. All of these applications
    require sophisticated probabilistic reasoning over complex, structured data and rely on probabilistic programming languages (PPLs) with
    programmable inference — the ability to customize probabilistic inference algorithms through proposals, kernels, and variational families —
    to improve the quality of posterior approximation. But fully realizing the benefits that probabilistic programming can deliver often requires
    substantial computational resources, as probabilistic inference scales by increasing the number of likelihood evaluations, sequential Monte Carlo
    particles, or Markov chain Monte Carlo chains.
  </p>

  <p>
    We present GenJAX, a new language and compiler for vectorized programmable probabilistic inference.
    GenJAX integrates the vectorizing map (<code>vmap</code>) operation from array programming frameworks such as JAX
    into the context of probabilistic programming with programmable inference, enabling the compositional vectorization of features such as
    probabilistic program traces, stochastic branching (for expressing mixture models), and programmable inference interfaces. This vectorization
    enables the implementation of compute-intensive probabilistic programming and probabilistic inference operations on modern GPUs, making it
    possible to deploy the substantial computational resources that GPUs provide to accelerate large-scale probabilistic inference.
  </p>

  <h3>Design Considerations</h3>
  <p>
    GenJAX is designed around the interaction between <code>vmap</code> and several probabilistic programming features that support the
    implementation of sophisticated models and inference algorithms:
  </p>

  <ul>
    <li><strong>Compositional vectorization.</strong> Our target class of probabilistic programs features multiple vectorizable computational patterns.
      Examples include computing likelihoods simultaneously on many pieces of data (as part of modeling) and evolving collections of particles or chains
      (as part of inference). Our integration of <code>vmap</code> therefore supports vectorization of both modeling and inference code.</li>
    <li><strong>Vectorization of probabilistic program traces.</strong> Traces are structured record objects used to represent samples. They are a data lingua franca
      for Monte Carlo and variational inference. Under vectorization by <code>vmap</code>, traces support an efficient struct-of-array representation.</li>
    <li><strong>Vectorized stochastic branching.</strong> Probabilistic mixture models, regime-switching dynamics models, and adaptive inference algorithms require
      stochastic branching using random values. GenJAX supports stochastic branching while maintaining vectorization.</li>
  </ul>

  <figure class="figure">
    <img src="static/images/fig1-essential-capabilities.svg" alt="Essential capabilities for vectorized probabilistic programs" loading="lazy" />
    <figcaption class="fig-caption"><strong>Figure.</strong> Computational patterns in vectorizable probabilistic programs.
      Left: Within models, vectorization can be used to parallelize conditionally independent computations. Within inference, vectorization can be used to
      simulate multiple particles in parallel. <code>vmap</code> should be applicable in both settings. Center: Traces are records used to represent samples
      from probabilistic programs. Both vectorized models and vectorized inference algorithms are designed to work with vectorized (struct-of-array) traces.
      Right: Probabilistic programs can branch on random values, and <code>vmap</code> of probabilistic programs should preserve this capability.</figcaption>
  </figure>

  <figure class="figure">
    <img src="static/images/fig2-intro-approach.svg" alt="GenJAX design overview" loading="lazy" />
    <figcaption class="fig-caption"><strong>Figure.</strong> The design and implementation of GenJAX.
      Left: GenJAX extends <code>vmap</code> to apply to both generative models and inference algorithms. Our system implements inference on a vectorized model by
      vectorizing inference applied to the model, which is justified by the commutativity corollary in the paper. Right: Survey of features in our language and compiler:
      usage of these features is illustrated in Section 2 and Section 7, and implementation is discussed in Section 6.</figcaption>
  </figure>

  <h3>Contributions</h3>
  <ol class="contributions">
    <li><strong>GenJAX: high-performance compiler (Section 6).</strong> GenJAX is an open-source compiler that extends JAX and <code>vmap</code> to support
      programmable probabilistic inference. Probabilistic programs in GenJAX can be systematically transformed to take advantage of opportunities for
      vectorization in both modeling and inference. Our compiler also eliminates the overhead present in many libraries for programmable inference: we implement
      simulation and density interfaces using lightweight effect handlers, and exploit JAX's support for program tracing to partially evaluate inference logic away at
      compile time, leaving only optimized array operations. Our design maintains full compatibility with JAX's underlying ecosystem for automatic differentiation and
      CPU/GPU/TPU compilation.</li>
    <li><strong>Formal model: interaction between <code>vmap</code> and programmable inference features (Section 3).</strong> We develop a formal model
      characterizing how <code>vmap</code> interacts with probabilistic program traces and programmable inference interfaces. We introduce $\gen$, a calculus for
      probabilistic programming and programmable inference, on top of a core probabilistic array language for stochastic parallel computations. We define <code>vmap</code>
      as a program transformation, prove its correctness, and show how it interacts with programmable inference interfaces to support vectorization of probabilistic
      computations and traces.</li>
    <li><strong>Empirical evaluation (Section 7).</strong> We evaluate our design and implementation through a series of benchmarks and case studies: performance
      comparison, where GenJAX achieves near-handcoded JAX performance and can outperform existing vectorized and high-performance PPLs and array programming
      frameworks (JAX, PyTorch, Pyro, NumPyro, and Gen); and high-dimensional vectorized inference, where we study approximate Game of Life inversion (find the previous
      512 x 512 board state which leads to the observed state) and sequential 2D robot localization with simulated LIDAR measurements. In both case studies, we use
      GenJAX to develop sophisticated vectorized inference algorithms, including vectorized Gibbs sampling and sequential Monte Carlo with vectorized proposals. Our final
      GenJAX programs exhibit high approximation accuracy and run in milliseconds on consumer-grade GPUs.</li>
  </ol>
</section>

</div><!-- end container -->

<!-- Track Switcher -->
<section class="track-switcher-section" aria-labelledby="track-heading">
  <div class="container">
    <h2 id="track-heading">Explore the Paper</h2>
    <p class="track-description">Choose your path through the material. The <strong>Tutorial</strong> track follows the paper's Overview section with a running polynomial regression example. The <strong>Theory</strong> track follows the Formal Model and Compiler sections with semantics, transformations, and proofs. <strong>All</strong> shows everything.</p>
    <div class="track-switcher" role="tablist" aria-label="Content tracks">
      <button class="track-btn tutorial-track active" id="btn-tutorial" 
              role="tab" aria-selected="true" aria-controls="track-tutorial" tabindex="0">
        <span class="track-icon" aria-hidden="true">
          <svg viewBox="0 0 24 24" focusable="false" aria-hidden="true">
            <path d="M12 6v14"/>
            <path d="M4 6h6a2 2 0 0 1 2 2v12H6a2 2 0 0 0-2 2V6z"/>
            <path d="M20 6h-6a2 2 0 0 0-2 2v12h6a2 2 0 0 1 2 2V6z"/>
          </svg>
        </span>
        <span class="track-label">Tutorial</span>
        <span class="track-desc">Progressive examples &amp; intuitive explanations</span>
      </button>
      <button class="track-btn theory-track" id="btn-theory"
              role="tab" aria-selected="false" aria-controls="track-theory" tabindex="-1">
        <span class="track-icon" aria-hidden="true">
          <svg viewBox="0 0 24 24" focusable="false" aria-hidden="true">
            <path d="M9 2h6"/>
            <path d="M10 2v6l-5.5 9.5a3 3 0 0 0 2.6 4.5h9.8a3 3 0 0 0 2.6-4.5L14 8V2"/>
            <path d="M8 12h8"/>
          </svg>
        </span>
        <span class="track-label">Theory</span>
        <span class="track-desc">Formal model, type systems &amp; proofs</span>
      </button>
      <button class="track-btn all-track" id="btn-all"
              role="tab" aria-selected="false" aria-controls="track-tutorial track-theory" tabindex="-1">
        <span class="track-icon" aria-hidden="true">
          <svg viewBox="0 0 24 24" focusable="false" aria-hidden="true">
            <path d="M12 3l9 5-9 5-9-5 9-5z"/>
            <path d="M3 12l9 5 9-5"/>
            <path d="M3 16l9 5 9-5"/>
          </svg>
        </span>
        <span class="track-label">All</span>
        <span class="track-desc">Show all content from both tracks</span>
      </button>
    </div>
  </div>
</section>

<div class="container container-wide">
<div class="main-content with-sidebar">

<!-- Content Area -->
<div class="content-area">
<!-- TUTORIAL TRACK CONTENT -->
<div id="track-tutorial" class="track-content active" role="tabpanel" aria-labelledby="btn-tutorial">

<section id="tutorial" data-nav-section aria-labelledby="tutorial-heading">
  <h2 id="tutorial-heading">Overview (Tutorial): Vectorized Probabilistic Programming <a href="#tutorial" class="section-anchor" aria-label="Link to Overview (Tutorial)">#</a></h2>

  <p>
    To introduce our language, consider the task of polynomial regression: given a dataset of pairs $(x_i, y_i) \in \mathbb{R}^2$
    for $1 \leq i \leq n$, we wish to infer a polynomial relating $x$ and $y$. In the following sections, we illustrate how to solve
    this problem using generative functions and programmable inference in GenJAX.
  </p>

  <section id="tutorial-core" class="tutorial-subsection" aria-labelledby="tutorial-core-heading">
    <h3 id="tutorial-core-heading">Vectorizing Generative Functions with <code>vmap</code></h3>

    <p>
      The first figure depicts a generative model for quadratic regression. The ultimate goal is to, given a noisy dataset
      $(x_i, y_i)_{1 \leq i \leq n}$, infer a quadratic function that plausibly governs the relationship between $x$ and $y$.
      Our model for this task is defined by composing generative functions, each defined as a <code>@gen</code>-decorated Python function.
      A key feature of GenJAX is that each random choice is assigned a string-valued name using the syntax <code>dist @ "name"</code>.
      The <code>polynomial</code> generative function describes a prior distribution on the coefficients (a, b, c) of the underlying quadratic function.
      The <code>point</code> generative function models how an individual datapoint is generated based on those coefficients. Finally, <code>npoint_curve</code>
      calls <code>polynomial</code> to generate coefficients and maps the <code>point</code> generative function over a vector of inputs.
    </p>

    <p>
      This is our first use of <code>vmap</code>: we use it to generate multiple y values in parallel, exploiting the fact that datapoints are generated
      conditionally independently of one another, given the coefficients. This is an instance of a general pattern that appears in many probabilistic programs,
      and is one key place where vectorization can yield significant speed-ups when parts of the generative model can be parallelized.
    </p>

    <figure class="figure">
      <div class="figure-row two-col">
        <div class="code-block">
          <div class="code-header">Generative functions</div>
          <pre><code class="language-python"># Basic polynomial model
@gen
def polynomial():
  # @ denotes introduction of
  # random choices
  a = normal(0, 1) @ "a"
  b = normal(0, 1) @ "b"
  c = normal(0, 1) @ "c"
  return (a, b, c)

# Point model with noise
@gen
def point(x, a, b, c):
  y_mean = a + b * x + c * x ** 2
  y = normal(y_mean, 0.2) @ "obs"
  return y</code></pre>
        </div>
        <div class="code-block">
          <div class="code-header">Vectorization with <code>vmap</code></div>
          <pre><code class="language-python">@gen
def npoint_curve(xs):
  (a, b, c) = polynomial() @ "curve"
  # Vectorization for modeling: here, over data points
  ys = point.vmap(args_mapped=0)(xs, a, b, c) @ "ys"
  return (a, b, c), ys

# Vectorized sampling from the generative function
# using the simulate interface.
xs = array([0.1, 0.3, 0.4, 0.6])
traces = vmap(simulate(npoint_curve), repeat=4)(xs)

# Vectorized evaluation of the pointwise density
# using the assess interface.
xs = traces.get_args()
densities, retvals = (
    vmap(assess(npoint_curve), args_mapped=0)(
        traces, xs
    )
)</code></pre>
        </div>
      </div>
      <figcaption class="fig-caption"><strong>Figure.</strong> Vectorization of generative functions.
        Left: Probabilistic programs encoding a prior over quadratic functions, and a single-datapoint likelihood.
        Right: <code>vmap</code> can be used to parallelize the likelihood: the same program that works for single points works for many points via <code>vmap</code>.
        Inference operations are also compatible with <code>vmap</code>.</figcaption>
    </figure>

    <p>
      When <code>vmap</code> transforms a generative function, it induces a transformation on the values in the trace, preserving the structure of the trace while
      converting scalars to arrays and returning a trace in struct-of-array representation.
    </p>
  </section>

  <section id="tutorial-step-1" class="tutorial-step" aria-labelledby="tutorial-step-1-heading">
    <h3 id="tutorial-step-1-heading">Vectorized Programmable Inference</h3>

    <p>
      Generative functions are compiled to implementations of the generative function interface, which includes methods like <code>simulate</code> and <code>assess</code>.
      The <code>simulate</code> method runs a generative function and yields an execution trace, and <code>assess</code> computes the probability density of a generative
      function at a given trace. These methods can be composed to implement inference algorithms. For example, likelihood weighting involves simulating many possible
      traces from the prior, and assessing them under the likelihood.
    </p>

    <p>
      By vectorizing the compiled <code>simulate</code> and <code>assess</code> methods, we can generate or assess many traces at once. We can use <code>vmap</code> to scale
      the number of guesses, automatically transforming single-particle code into a many-particle vectorized version. If the algorithm is executed in parallel on a GPU,
      the number of particles can be freely increased as long as the GPU has free memory. The time remains near constant as we increase the number of particles, and the
      accuracy improves to convergence.
    </p>

    <figure class="figure">
      <div class="figure-row two-col">
        <div class="code-block">
          <div class="code-header">Single particle importance sampling</div>
          <pre><code class="language-python"># Single particle importance sampling.
def importance_sampling(ys, xs):
  trace = simulate(default_proposal)(xs)
  logp, _ = assess(npoint_curve)(
      {"ys" : {"obs" : ys}},
      xs
  )
  w = logp - trace.get_score()
  return (trace, w)</code></pre>
        </div>
        <div class="code-block">
          <div class="code-header">Vectorized over N particles</div>
          <pre><code class="language-python"># Vectorized over N particles.
def vectorized_importance_sampling(ys, xs, N):
  # vmap automatically batches over n copies
  return vmap(
      importance_sampling,
      repeat=N
  )(ys, xs)

# Compute log marginal likelihood estimate.
def lmle(ws, N):
  return logsumexp(ws) - log(N)</code></pre>
        </div>
      </div>
      <img src="static/images/curvefit_scaling_performance.svg"
           alt="Scaling behavior of vectorized importance sampling"
           loading="lazy" />
      <img src="static/images/curvefit_posterior_scaling_combined.svg"
           alt="Posterior approximations at different particle counts"
           loading="lazy" />
      <figcaption class="fig-caption"><strong>Figure.</strong> Vectorized programmable inference.
        Top left: Single-particle importance sampling with a proposal (the default proposal is the prior in the <code>npoint_curve</code> model, excluding the <code>"obs"</code>
        random variable) implemented using generative function interface methods (<code>simulate</code> and <code>assess</code>). Top right: Using <code>vmap</code>, we can automatically
        transform the single-particle version into a many-particle vectorized version. Middle: The vectorized version runs in parallel on GPUs; the runtime is nearly constant
        as long as the GPU has memory to spare. Increasing the number of particles increases accuracy. Bottom: Posterior approximations for different numbers of particles N.</figcaption>
    </figure>
  </section>

  <section id="tutorial-step-2" class="tutorial-step" aria-labelledby="tutorial-step-2-heading">
    <h3 id="tutorial-step-2-heading">Improving Robustness Using Stochastic Branching</h3>

    <p>
      In real-world data, the assumptions of simple polynomial regression are often violated. Our polynomial model assumes every data point follows the same noise model —
      but what if 10% of our measurements follow a different distribution? We can improve robustness by using stochastic branching, which allows us to account for outlier
      observations through heterogeneous mixture modeling. Each data point gets a latent outlier flag. If the flag is true, the observation comes from a uniform distribution;
      if false, it follows the noisy polynomial curve.
    </p>

    <figure class="figure">
      <div class="figure-row two-col">
        <div class="code-block">
          <div class="code-header">Outlier-robust observation model</div>
          <pre><code class="language-python"># Outlier-robust observation model
@gen
def point_with_outliers(x, a, b, c):
  outlier_flag = bernoulli(0.1) @ "outlier"
  y_mean = a + b * x + c * x ** 2
  return cond(outlier_flag,
    lambda x: uniform(-2.0, 2.0),
    lambda x: trunc_norm(x, 0.05, 2.0),
    y_mean,
  ) @ "obs"</code></pre>
        </div>
        <div class="code-block">
          <div class="code-header">Vectorized curve model with outliers</div>
          <pre><code class="language-python"># Vectorized curve model with outliers
@gen
def npoint_curve_with_outliers(xs):
  (a, b, c) = polynomial() @ "curve"
  ys = point_with_outliers.vmap(
        args_mapped=0,
    )(xs, a, b, c) @ "ys"
  return ys</code></pre>
        </div>
      </div>
      <img src="static/images/curvefit_outlier_detection_comparison.svg"
           alt="Outlier detection comparison"
           loading="lazy" />
      <figcaption class="fig-caption"><strong>Figure.</strong> Robust modeling with stochastic branching.
        Stochastic branching allows us to extend our models to explain more complex data, including data with outliers. Circle markers depict observed data points; the shading
        of the marker denotes the estimated posterior probability that the point is an outlier. Bottom, left: Using importance sampling to construct a posterior in our original
        model results in a poor explanation of the data. Bottom, middle: Extending the model to explicitly represent outliers as random variables should allow us to produce better
        explanations, but results in a harder inference problem which importance sampling cannot effectively solve. Bottom, right: Changing inference to vectorized MCMC using Gibbs
        sampling (to infer outliers) and Hamiltonian Monte Carlo (to infer continuous parameters) finds better explanations of the data, i.e., more accurate posterior approximations.</figcaption>
    </figure>
  </section>

  <section id="tutorial-step-3" class="tutorial-step" aria-labelledby="tutorial-step-3-heading">
    <h3 id="tutorial-step-3-heading">Improving Inference Accuracy Using Programmable Inference</h3>

    <p>
      Even when a model's assumptions are sensible, inference can fail to find good explanations of a given dataset. Importance sampling applied to the outlier model identifies
      likely outliers, but has wide uncertainty over the possible curves, and several curves do not seem to explain the data well. This is a kind of underfitting: by adding new
      latent variables to our model, we have made inference more challenging, and the "guess and check" approach of importance sampling runs into limitations, even with $N = 10^5$
      particles — the limit where our GPU memory begins to saturate.
    </p>

    <p>
      The right panel of the outlier figure illustrates the results of a custom hybrid algorithm, which combines Gibbs sampling and Hamiltonian Monte Carlo (HMC). The algorithm
      uses Gibbs sampling to identify which points are outliers, and HMC to sample from the posterior distribution over curves, given the inliers. This algorithm generates much
      more accurate posterior samples that explain the data well.
    </p>
  </section>

  <section id="tutorial-step-4" class="tutorial-step" aria-labelledby="tutorial-step-4-heading">
    <h3 id="tutorial-step-4-heading">Vectorized Gibbs Sampling and the Generative Function Interface</h3>

    <p>
      We present the GenJAX implementation of the Gibbs sampling step of our hybrid algorithm. Our implementation highlights generative function interface methods, including
      trace manipulation and getter methods. In our outlier model, we apply Gibbs sampling to update the vector of outlier choices, keeping other random choices constant. As each
      outlier choice is conditionally independent from the others (given all the non-outlier choices), the outlier updates can be vectorized. For each element, we enumerate the
      unnormalized posterior density for the possible values for the outlier value, and then sample a new value from a categorical distribution.
    </p>

    <figure class="figure">
      <div class="figure-row two-col">
        <div class="code-block">
          <div class="code-header">Enumerative Gibbs update for single point</div>
          <pre><code class="language-python">def gibbs_outlier(subtrace):
  def _assess(v):
    (x, a, b, c) = subtrace.args()
    chm = {"outlier": v,
           "obs": subtrace["obs"]}
    log_prob, _ = assess(point_with_outliers)(
      chm, x, a, b, c
    )
    return log_prob

  log_probs = vmap(_assess)(
    array([False, True])
  )
  return categorical(log_probs) == 1</code></pre>
        </div>
        <div class="code-block">
          <div class="code-header">Vectorized enumerative Gibbs</div>
          <pre><code class="language-python"># `trace` is a single trace object
# whose fields store batched values.
def enumerative_gibbs(trace):
  xs  = trace.get_args()
  # `subtrace` refers to the struct-of-arrays
  # view for the "ys" addresses.
  subtrace = trace.get_subtrace("ys")
  new_outliers = vmap(gibbs_outlier)(subtrace)
  # `update` applies the generative function
  # interface method that edits a trace.
  new_trace, weight, _ = update(
    trace,
    {"ys": {"outlier": new_outliers}},
  )
  return new_trace</code></pre>
        </div>
      </div>
      <figcaption class="fig-caption"><strong>Figure.</strong> Vectorized enumerative Gibbs sampling for outlier detection.
        Left: Enumerative Gibbs update for a single data point's outlier indicator. For each possible value (inlier/outlier), we compute the log probability under the model
        (proportional to the unnormalized posterior) and sample a new indicator using categorical sampling. Right: Vectorized Gibbs sampling step that applies the single-point
        update across all data points using <code>vmap</code>, then updates the trace with the new outlier indicators.</figcaption>
    </figure>

    <figure class="figure">
      <div class="figure-row two-col">
        <div class="code-block">
          <div class="code-header">simulate: sampling</div>
          <pre><code class="language-python"># Unconstrained sampling of a trace
tr = simulate(npoint_curve)(xs)</code></pre>
        </div>
        <div class="code-block">
          <div class="code-header">assess: density evaluation</div>
          <pre><code class="language-python"># Evaluate log density at traced sample
chm = get_choices(tr)
logp, retval = assess(npoint_curve)(chm, xs)</code></pre>
        </div>
      </div>
      <div class="figure-row two-col">
        <div class="code-block">
          <div class="code-header">generate: importance sampling</div>
          <pre><code class="language-python"># Constrained sampling of a trace
partial_chm = {"ys": {"obs": data}}
tr_, weight = generate(npoint_curve)(
    partial_chm, xs
)</code></pre>
        </div>
        <div class="code-block">
          <div class="code-header">update: trace modification</div>
          <pre><code class="language-python"># Modify a trace given constraints
new_chm = {"curve": {"a": 1.0}}
tr_, w, discard = update(npoint_curve)(
    tr, new_chm, xs
)</code></pre>
        </div>
      </div>
      <figcaption class="fig-caption"><strong>Figure.</strong> Generative function interface methods.
        GenJAX's generative functions provide several methods for programmable inference — a way to extend the system with new variants of inference using high-level interfaces.
        For authoring programmable algorithms which use proposal distributions (like sequential Monte Carlo), the <code>simulate</code> method performs unconstrained sampling and
        reciprocal density evaluation. For density evaluation, <code>assess</code> evaluates the log joint density of a generative function on traced samples. The <code>generate</code>
        interface performs constrained sampling (using importance weighting), allowing construction of a trace with observation constraints. The <code>update</code> method modifies a
        trace with provided choices, returning an updated trace and an incremental importance weight, and is used by algorithms like Gibbs sampling or Hamiltonian Monte Carlo to
        modify traces.</figcaption>
    </figure>
  </section>
</section>

</div><!-- end tutorial track -->

<!-- THEORY TRACK CONTENT -->
<div id="track-theory" class="track-content" role="tabpanel" aria-labelledby="btn-theory">

<section id="theory" data-nav-section aria-labelledby="theory-heading">
  <h2 id="theory-heading">Formal Model (Theory): $\gen$ — A Core Calculus <a href="#theory" class="section-anchor" aria-label="Link to Formal Model (Theory)">#</a></h2>

  <p>
    In this section, we give the syntax and semantics of a core calculus for traced probabilistic programming with vectors, and formalize a program transformation that
    vectorizes probabilistic programs. The formal model distills key ideas from our actual implementation in JAX, described in Section 6.
  </p>

  <section id="theory-syntax" aria-labelledby="theory-syntax-heading">
    <h3 id="theory-syntax-heading">Syntax of $\gen$</h3>
    <p>
      $\gen$ is a simply-typed lambda calculus which extends a standard array programming calculus in two main ways: (1) a probability monad for stochastic computations; and
      (2) a graded monad of generative functions, or traced probabilistic programs. Generative functions can be automatically compiled to the density functions and stochastic
      traced simulation procedures necessary for inference.
    </p>
  </section>

  <section id="theory-semantics" aria-labelledby="theory-semantics-heading">
    <h3 id="theory-semantics-heading">Denotational Semantics</h3>
    <p>
      We give a denotational semantics for $\gen$ using quasi-Borel spaces (QBS), a standard mathematical framework for higher-order probabilistic programming. We assign to
      each type a space and to each term a map from the interpretation of the environment to the interpretation of its return type. A generative function is interpreted as a
      pair of a measure on traces and a return value function that computes the program's output given values for all random choices.
    </p>
  </section>

  <section id="theory-inference" aria-labelledby="theory-inference-heading">
    <h3 id="theory-inference-heading">Programmable Inference Transformations</h3>
    <p>
      The formal model characterizes how generative functions support programmable inference by compiling to simulation and density-evaluation procedures. These transformations
      provide the foundation for implementing inference algorithms via high-level interfaces such as <code>simulate</code>, <code>assess</code>, <code>generate</code>, and
      <code>update</code>, which we use throughout the system.
    </p>
  </section>

  <section id="theory-vectorization" aria-labelledby="theory-vectorization-heading">
    <h3 id="theory-vectorization-heading">Vectorization as Program Transformation</h3>
    <p>
      We introduce <code>vmap</code> as a program transform for vectorization and prove its correctness for deterministic, probabilistic, and generative computations. The proofs
      show how vectorization preserves distributions and trace structure and how it interacts with programmable inference interfaces.
    </p>
  </section>

  <section id="theory-compiler" aria-labelledby="theory-compiler-heading">
    <h3 id="theory-compiler-heading">Compiler Connection</h3>
    <p>
      The formal results justify the implementation strategy in the compiler section: inference on a vectorized model can be implemented by vectorizing inference applied to the
      model. Section 6 describes the compiler architecture and how these ideas are realized in a JAX-based implementation.
    </p>
  </section>
</section>

</div><!-- end theory track -->

</div><!-- end content-area -->
<!-- Sidebar Navigation -->
<aside class="sidebar" aria-label="Table of contents">
  <div class="toc-title">On this page</div>
  <nav class="toc-nav" aria-label="Page sections">
    <ul>
      <li><a href="#abstract" class="toc-h2">Abstract</a></li>
      <li><a href="#introduction" class="toc-h2">Introduction</a></li>
      <li data-track="tutorial"><a href="#tutorial" class="toc-h2">Overview (Tutorial)</a>
        <ul>
          <li><a href="#tutorial-core" class="toc-h3">Vectorizing Generative Functions</a></li>
          <li><a href="#tutorial-step-1" class="toc-h3">Vectorized Inference</a></li>
          <li><a href="#tutorial-step-2" class="toc-h3">Stochastic Branching</a></li>
          <li><a href="#tutorial-step-3" class="toc-h3">Programmable Inference</a></li>
          <li><a href="#tutorial-step-4" class="toc-h3">Vectorized Gibbs + GFI</a></li>
        </ul>
      </li>
      <li data-track="theory"><a href="#theory" class="toc-h2">Formal Model (Theory)</a>
        <ul>
          <li><a href="#theory-syntax" class="toc-h3">Syntax of $\gen$</a></li>
          <li><a href="#theory-semantics" class="toc-h3">Denotational Semantics</a></li>
          <li><a href="#theory-inference" class="toc-h3">Inference Transformations</a></li>
          <li><a href="#theory-vectorization" class="toc-h3">Vectorization Transform</a></li>
          <li><a href="#theory-compiler" class="toc-h3">Compiler Connection</a></li>
        </ul>
      </li>
      <li><a href="#evaluation" class="toc-h2">Evaluation</a>
        <ul>
          <li><a href="#evaluation-performance" class="toc-h3">Performance</a></li>
          <li><a href="#evaluation-gol" class="toc-h3">Game of Life</a></li>
          <li><a href="#evaluation-localization" class="toc-h3">Localization</a></li>
        </ul>
      </li>
      <li><a href="#conclusion" class="toc-h2">Conclusion</a></li>
      <li><a href="#artifact" class="toc-h2">Artifact</a></li>
      <li><a href="#citation" class="toc-h2">Citation</a></li>
    </ul>
  </nav>
  
  <div class="track-widget" style="margin-top: var(--space-2xl); padding-top: var(--space-xl); border-top: 1px solid var(--color-border);">
    <div class="toc-title">View Mode</div>
    <div class="track-widget-buttons">
      <button type="button" class="track-widget-btn" onclick="if(window.GenJAX) { window.GenJAX.switchTrack('tutorial'); }">
        <span class="track-dot" style="background: var(--color-tutorial);"></span> Tutorial
      </button>
      <button type="button" class="track-widget-btn" onclick="if(window.GenJAX) { window.GenJAX.switchTrack('theory'); }">
        <span class="track-dot" style="background: var(--color-theory);"></span> Theory
      </button>
      <button type="button" class="track-widget-btn" onclick="if(window.GenJAX) { window.GenJAX.switchTrack('all'); }">
        <span class="track-dot" style="background: var(--color-all);"></span> All
      </button>
    </div>
  </div>
</aside>

</div><!-- end main-content -->

<!-- EVALUATION (Common section) -->
<section id="evaluation" data-nav-section aria-labelledby="evaluation-heading">
  <h2 id="evaluation-heading">Evaluation <a href="#evaluation" class="section-anchor" aria-label="Link to Evaluation">#</a></h2>

  <p>
    We evaluate our language and compiler implementation on benchmarks and case studies designed to assess the following criteria:
  </p>

  <ul>
    <li><strong>Performance.</strong> How does the performance of our compiler implementation compare to leading programmable inference systems? Do our abstractions introduce
      overhead compared to handcoded implementations of inference? We survey the performance properties of GenJAX against open-source PPLs and tensor frameworks on standard
      modeling and inference tasks, for both embarrassingly-parallel algorithms (importance sampling) and iterative differentiable algorithms (Hamiltonian Monte Carlo).</li>
    <li><strong>Inference quality.</strong> <code>vmap</code> provides a convenient way to express inference problems over high-dimensional spaces. We study probabilistic Game of
      Life inversion on large boards using approximate inference and use GenJAX to construct an efficient nested vectorized Gibbs sampler. We also study a probabilistic model of
      robot localization using simulated LIDAR measurements and construct sequential Monte Carlo algorithms, including an efficient algorithm using proposals with vectorized
      locally optimal grid approximations.</li>
  </ul>

  <section id="evaluation-performance" aria-labelledby="evaluation-performance-heading">
    <h3 id="evaluation-performance-heading">Performance Survey Evaluation</h3>

    <figure class="figure">
      <img src="static/images/combined_3x2_obs50_samples2000.svg"
           alt="Beta-Bernoulli inference accuracy and timing comparison"
           loading="lazy" />
      <div class="figure-row two-col">
        <div>
          <img src="static/images/benchmark_timings_is_all_frameworks.svg"
               alt="Importance sampling timing comparison"
               loading="lazy" />
        </div>
        <div>
          <img src="static/images/benchmark_timings_hmc_all_frameworks.svg"
               alt="HMC timing comparison"
               loading="lazy" />
        </div>
      </div>
      <figcaption class="fig-caption"><strong>Figure.</strong> Performance evaluation across probabilistic programming frameworks.
        (a) Beta-Bernoulli inference comparing posterior accuracy and execution time for GenJAX, NumPyro, and handcoded JAX implementations with 50 observations and 2000 samples,
        demonstrating importance sampling using GenJAX's programmable inference abstractions is competitive with handcoded performance.
        (b) Scaling analysis across six frameworks showing GenJAX achieves performance is consistently near handcoded JAX and competitive with other open-source PPLs, for both
        importance sampling and Hamiltonian Monte Carlo.</figcaption>
    </figure>

    <p>
      The top panel examines the runtime characteristics of our compiler on importance sampling in a Beta-Bernoulli model. The model infers the bias of a coin from observed flips,
      using a Beta(1,1) prior and Bernoulli likelihood. We observe 50 flips and construct a posterior approximation using importance sampling. The results confirm that all frameworks
      accurately recover the true posterior distribution. GenJAX achieves near-identical performance to handcoded JAX (100.1% relative time).
    </p>

    <p>
      The bottom panel presents performance results for importance sampling and Hamiltonian Monte Carlo (HMC) on the polynomial regression problem from the overview. Importance
      sampling exhibits parallel scaling with the number of particles: vectorized PPLs and tensor frameworks have near constant scaling while the GPU is not saturated. HMC is run
      iteratively, so scaling is linear in the length of the chain. GenJAX is consistently close to handcoded and optimized JAX, validating that our abstractions for programmable
      inference introduce minimal overhead.
    </p>
  </section>

  <section id="evaluation-gol" aria-labelledby="evaluation-gol-heading">
    <h3 id="evaluation-gol-heading">Probabilistic Game of Life Inversion</h3>

    <figure class="figure">
      <img src="static/images/conway_gol.svg"
           alt="Conway's Game of Life evolution rules"
           loading="lazy" />
      <figcaption class="fig-caption"><strong>Figure.</strong> Deterministic evolution rules for Conway's <em>Game of Life</em>.
        In our case study, we add a uniform prior over board state and probabilistic Bernoulli noise on top of the deterministic rules to construct a <em>Game of Life</em>
        generative function. We can then condition the observed next state and construct an inference problem whose solutions correspond to approximate inversions of the
        Game of Life dynamics.</figcaption>
    </figure>

    <figure class="figure">
      <img src="static/images/gol_integrated_showcase_wizards_1024.svg"
           alt="Game of Life inversion showcase"
           loading="lazy" />
      <img src="static/images/gol_gibbs_timing_bar_plot.svg"
           alt="Game of Life Gibbs sampling throughput"
           loading="lazy" />
      <figcaption class="fig-caption"><strong>Figure.</strong> Vectorized Gibbs sampling in the <em>Game of Life</em>.
        Probabilistic Game of Life inversion on the wizard book cover ($1024 \times 1024$ grid). Top: (1) Previous state (unknown, the target of our inference process);
        (2) Observed future state (the target pattern); (3) Vectorized Gibbs chain showing states constructed by inference in a progression from t=0 to t=499;
        (4) One-step deterministic evolution of final inferred state, reconstruction accuracy (measured as discrepancy between bits) is around 90%.
        Bottom: Benchmark timings of single vectorized Gibbs sweep performance across board sizes, comparing CPU and GPU execution times. GPU execution timings demonstrate the benefit
        of parallel hardware acceleration for vectorized inference. Overall: the runtime takes about 2.8 seconds for 500 iterations on an RTX 4090 GPU, with about 93% reconstruction
        accuracy (70,109 bits out of 1,048,576 total bits).</figcaption>
    </figure>

    <p>
      Game of Life inversion is the problem of inverting the dynamics of Conway's <em>Game of Life</em>: given a final state, what is a possible previous state that evolves to
      the final state under the rules of the game? Brute force discrete search is computationally intractable, requiring evaluation of $2^{N \times N}$ states, where $N$ is the linear
      dimension of a square Game of Life board. In this case study, we introduce probabilistic noise into the dynamics: from an initial state, we evolve forward using the
      deterministic rules, but then sample with Bernoulli noise around the true value of each pixel. We illustrate approximate inversion using vectorized Gibbs sampling.
    </p>

    <p>
      Because each cell's value is conditionally independent from non-neighboring cells' values, given its eight neighbors, we partition the board's cells into conditionally
      independent groups (given the other groups). Within each group, we can perform parallel Gibbs updates on all the cells, an example of chromatic Gibbs. The result is a highly
      efficient probabilistic inversion algorithm which can invert Life states with up to 90% accuracy in a few seconds.
    </p>
  </section>

  <section id="evaluation-localization" aria-labelledby="evaluation-localization-heading">
    <h3 id="evaluation-localization-heading">Robot Localization</h3>

    <figure class="figure">
      <img src="static/images/localization_r8_p200_basic_localization_problem_1x4_explanation.svg"
           alt="Robot localization problem"
           loading="lazy" />
      <img src="static/images/localization_r8_p200_basic_comprehensive_4panel_smc_methods_analysis.svg"
           alt="SMC methods comparison"
           loading="lazy" />
      <figcaption class="fig-caption"><strong>Figure.</strong> Robot localization using programmable sequential Monte Carlo.
        (a) Problem setup showing robot trajectory through a multi-room environment with an 8-ray LIDAR sensor model for distance-based localization.
        (b) Comparison of three SMC variants: Bootstrap filter, SMC+HMC, and SMC+Locally Optimal, showing particle approximation evolution and execution time performance.</figcaption>
    </figure>

    <p>
      In robotics, simultaneous mapping and localization (SLAM) refers to the problem of constructing a representation of the map of an environment and the position of the robot
      within the map based on measurements. If the map is given, the problem is called localization: a robot maneuvers through a known space and receives measurements of distance
      to the walls. The goal is to construct a probabilistic representation of where the robot is located. In this case study, we use GenJAX to write a model for robot
      localization, with Gaussian drift dynamics and a simulated LIDAR measurement. Given a sequence of LIDAR measurements over time as observations, we constrain the model to
      produce a posterior over robot locations.
    </p>

    <ul>
      <li><strong>Bootstrap filter.</strong> Sequential Monte Carlo where the prior (from the model) is used as the proposal for the latent position of the robot.</li>
      <li><strong>SMC + HMC.</strong> Adds HMC moves to the bootstrap filter. These moves are applied to the particle collection after resampling.</li>
      <li><strong>SMC + Locally Optimal.</strong> Uses a smart proposal based on enumerative grids: enumerate a grid in position space, evaluate each position against the
        observation likelihood, select the maximum likelihood grid point, and sample from a normal distribution around that point.</li>
    </ul>

    <p>
      SMC supports natural vectorization over particles. In our experiments, the best algorithm from the standpoint of efficiency and accuracy is locally optimal SMC, which adds
      another layer of vectorization within the custom proposal. The likelihood grid evaluations can be fully vectorized, and the model already features vectorization in the LIDAR
      measurement model. These opportunities for vectorization (in the model, in the locally optimal proposal, and across the particle collection) are convenient to program against
      with <code>vmap</code> and lead to a highly efficient inference algorithm which can accurately track the 2D robot's location within the map in milliseconds.
    </p>
  </section>
</section>


<!-- Conclusion -->
<section id="conclusion" data-nav-section aria-labelledby="conclusion-heading">
  <h2 id="conclusion-heading">Conclusion <a href="#conclusion" class="section-anchor" aria-label="Link to Conclusion">#</a></h2>

  <p>
    This work presents GenJAX, a language and compiler for vectorized probabilistic programming with programmable inference. This system integrates <code>vmap</code> with
    programmable inference features: we extend <code>vmap</code> support to generative functions, including support for vectorization using <code>vmap</code> of probabilistic
    program traces, stochastic branching, and programmable inference interfaces. Benchmarks show this approach yields low overhead relative to hand-optimized JAX, and
    simultaneously delivers greater expressiveness and competitive performance with other probabilistic programming systems targeting modern accelerators.
  </p>

  <h3>Future Work</h3>
  <ul>
    <li><strong>Vectorized inference diagnostics.</strong> By automating the vectorized implementation of nested models and inference algorithms, GenJAX makes it easy to experiment
      with parallel implementations of custom Monte Carlo estimators of a broad range of information-theoretic quantities derived from probabilistic programs, including KL divergence
      between inference algorithms and the conditional mutual information among subsets of latent variables. Although computationally intensive on CPUs, these estimators are comprised
      of nested, massively parallel computations, and may become more practical and widespread given suitable automation.</li>
    <li><strong>Spatial or geometric probabilistic programs.</strong> We expect that GenJAX's support for array programming and programmable probabilistic inference may be well-suited
      for spatial computing applications. Domains such as robotics, autonomous navigation, computational imaging, and scientific simulation increasingly require sophisticated
      probabilistic reasoning over high-dimensional spatial data — including LiDAR point clouds, depth images, and other spatial data types. Probabilistic programming applications in
      these domains naturally involve computations that manipulate multi-dimensional arrays. GenJAX's design is uniquely suited to support practitioners writing these types of
      probabilistic programs and provides useful vectorization automation and support for compilation to efficient GPU implementations.</li>
  </ul>
</section>

<!-- Artifact -->
<section id="artifact" data-nav-section aria-labelledby="artifact-heading">
  <h2 id="artifact-heading">Artifact & Data Availability <a href="#artifact" class="section-anchor" aria-label="Link to Artifact">#</a></h2>

  <p>
    The artifact associated with this paper is available on <a href="https://doi.org/10.5281/zenodo.17594132">Zenodo</a>.
    The source code is available at <a href="https://github.com/probcomp/genjax">https://github.com/probcomp/genjax</a>.
  </p>
</section>

<!-- Citation -->
<section id="citation" data-nav-section aria-labelledby="citation-heading">
  <h2 id="citation-heading">Citation <a href="#citation" class="section-anchor" aria-label="Link to Citation">#</a></h2>
  
  <div class="bibtex-container">
    <button class="copy-btn" onclick="copyBibtex()" aria-label="Copy citation to clipboard">Copy</button>
    <pre class="bibtex-block" id="bibtex"><code>@article{becker2026genjax,
  title     = {Probabilistic Programming with Vectorized Programmable Inference},
  author    = {Becker, McCoy R. and Huot, Mathieu and Matheos, George and
               Wang, Xiaoyan and Chung, Karen and Smith, Colin and
               Ritchie, Sam and Saurous, Rif A. and Lew, Alexander K. and
               Rinard, Martin C. and Mansinghka, Vikash K.},
  journal   = {Proceedings of the ACM on Programming Languages},
  volume    = {10},
  number    = {POPL},
  articleno = {87},
  year      = {2026},
  publisher = {ACM},
  doi       = {10.1145/3776729},
}</code></pre>
  </div>
</section>

</div><!-- end container -->
</main>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <p>
      GenJAX is developed at the
      <a href="https://probcomp.csail.mit.edu/">MIT Probabilistic Computing Project</a>.
    </p>
    <p>
      Copyright 2026 The Authors. Published in PACMPL.
    </p>
  </div>
</footer>

<!-- Inline script for functions that need to be global for onclick handlers -->
<script>
  // Copy BibTeX function (used by onclick handler)
  function copyBibtex() {
    const bibtex = document.getElementById('bibtex').textContent;
    navigator.clipboard.writeText(bibtex).then(() => {
      const btn = document.querySelector('.copy-btn');
      const originalText = btn.textContent;
      btn.textContent = 'Copied!';
      setTimeout(() => btn.textContent = originalText, 2000);
    });
  }
</script>

</body>
</html>
