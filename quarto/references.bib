
@article{lew_adev_2023,
	title = {{ADEV}: {Sound} {Automatic} {Differentiation} of {Expected} {Values} of {Probabilistic} {Programs}},
	volume = {7},
	shorttitle = {{ADEV}},
	url = {https://dl.acm.org/doi/10.1145/3571198},
	doi = {10.1145/3571198},
	abstract = {Optimizing the expected values of probabilistic processes is a central problem in computer science and its applications, arising in fields ranging from artificial intelligence to operations research to statistical computing. Unfortunately, automatic differentiation techniques developed for deterministic programs do not in general compute the correct gradients needed for widely used solutions based on gradient-based optimization.  

In this paper, we present ADEV, an extension to forward-mode AD that correctly differentiates the expectations of probabilistic processes represented as programs that make random choices. Our algorithm is a source-to-source program transformation on an expressive, higher-order language for probabilistic computation, with both discrete and continuous probability distributions. The result of our transformation is a new probabilistic program, whose expected return value is the derivative of the original program’s expectation. This output program can be run to generate unbiased Monte Carlo estimates of the desired gradient, that can be used within the inner loop of stochastic gradient descent. We prove ADEV correct using logical relations over the denotations of the source and target probabilistic programs. Because it modularly extends forward-mode AD, our algorithm lends itself to a concise implementation strategy, which we exploit to develop a prototype in just a few dozen lines of Haskell (https://github.com/probcomp/adev).},
	number = {POPL},
	urldate = {2025-04-17},
	journal = {Proc. ACM Program. Lang.},
	author = {Lew, Alexander K. and Huot, Mathieu and Staton, Sam and Mansinghka, Vikash K.},
	month = jan,
	year = {2023},
	pages = {5:121--5:153},
	file = {Full Text PDF:/Users/femtomc/Zotero/storage/SGDCPBKV/Lew et al. - 2023 - ADEV Sound Automatic Differentiation of Expected Values of Probabilistic Programs.pdf:application/pdf},
}

@article{becker_probabilistic_2024,
	title = {Probabilistic {Programming} with {Programmable} {Variational} {Inference}},
	volume = {8},
	url = {https://dl.acm.org/doi/10.1145/3656463},
	doi = {10.1145/3656463},
	abstract = {Compared to the wide array of advanced Monte Carlo methods supported by modern probabilistic programming languages (PPLs), PPL support for variational inference (VI) is less developed: users are typically limited to a predefined selection of variational objectives and gradient estimators, which are implemented monolithically (and without formal correctness arguments) in PPL backends. In this paper, we propose a more modular approach to supporting variational inference in PPLs, based on compositional program transformation. In our approach, variational objectives are expressed as programs, that may employ first-class constructs for computing densities of and expected values under user-defined models and variational families. We then transform these programs systematically into unbiased gradient estimators for optimizing the objectives they define. Our design makes it possible to prove unbiasedness by reasoning modularly about many interacting concerns in PPL implementations of variational inference, including automatic differentiation, density accumulation, tracing, and the application of unbiased gradient estimation strategies. Additionally, relative to existing support for VI in PPLs, our design increases expressiveness along three axes: (1) it supports an open-ended set of user-defined variational objectives, rather than a fixed menu of options; (2) it supports a combinatorial space of gradient estimation strategies, many not automated by today’s PPLs; and (3) it supports a broader class of models and variational families, because it supports constructs for approximate marginalization and normalization (previously introduced for Monte Carlo inference). We implement our approach in an extension to the Gen probabilistic programming system (genjax.vi, implemented in JAX), and evaluate our automation on several deep generative modeling tasks, showing minimal performance overhead vs. hand-coded implementations and performance competitive with well-established open-source PPLs.},
	number = {PLDI},
	urldate = {2025-04-17},
	journal = {Reproduction Packager for Article "Probabilistic Programming with Programmable Variational Inference"},
	author = {Becker, McCoy R. and Lew, Alexander K. and Wang, Xiaoyan and Ghavami, Matin and Huot, Mathieu and Rinard, Martin C. and Mansinghka, Vikash K.},
	month = jun,
	year = {2024},
	pages = {233:2123--233:2147},
	file = {Full Text PDF:/Users/femtomc/Zotero/storage/4CYVECEV/Becker et al. - 2024 - Probabilistic Programming with Programmable Variational Inference.pdf:application/pdf},
}

@article{lew_probabilistic_2023,
	title = {Probabilistic {Programming} with {Stochastic} {Probabilities}},
	volume = {7},
	url = {https://dl.acm.org/doi/10.1145/3591290},
	doi = {10.1145/3591290},
	abstract = {We present a new approach to the design and implementation of probabilistic programming languages (PPLs), based on the idea of stochastically estimating the probability density ratios necessary for probabilistic inference. By relaxing the usual PPL design constraint that these densities be computed exactly, we are able to eliminate many common restrictions in current PPLs, to deliver a language that, for the first time, simultaneously supports first-class constructs for marginalization and nested inference, unrestricted stochastic control flow, continuous and discrete sampling, and programmable inference with custom proposals. At the heart of our approach is a new technique for compiling these expressive probabilistic programs into randomized algorithms for unbiasedly estimating their densities and density reciprocals. We employ these stochastic probability estimators within modified Monte Carlo inference algorithms that are guaranteed to be sound despite their reliance on inexact estimates of density ratios. We establish the correctness of our compiler using logical relations over the semantics of λSP, a new core calculus for modeling and inference with stochastic probabilities. We also implement our approach in an open-source extension to Gen, called GenSP, and evaluate it on six challenging inference problems adapted from the modeling and inference literature. We find that: (1)  ‍can automate fast density estimators for programs with very expensive exact densities; (2) convergence of inference is mostly unaffected by the noise from these estimators; and (3) our sound-by-construction estimators are competitive with hand-coded density estimators, incurring only a small constant-factor overhead.},
	number = {PLDI},
	urldate = {2025-04-17},
	journal = {Proc. ACM Program. Lang.},
	author = {Lew, Alexander K. and Ghavamizadeh, Matin and Rinard, Martin C. and Mansinghka, Vikash K.},
	month = jun,
	year = {2023},
	pages = {176:1708--176:1732},
	file = {Full Text PDF:/Users/femtomc/Zotero/storage/B9TTUIQB/Lew et al. - 2023 - Probabilistic Programming with Stochastic Probabilities.pdf:application/pdf},
}

@inproceedings{cusumano-towner_gen_2019,
	address = {New York, NY, USA},
	series = {{PLDI} 2019},
	title = {Gen: a general-purpose probabilistic programming system with programmable inference},
	isbn = {978-1-4503-6712-7},
	shorttitle = {Gen},
	url = {https://dl.acm.org/doi/10.1145/3314221.3314642},
	doi = {10.1145/3314221.3314642},
	abstract = {Although probabilistic programming is widely used for some restricted classes of statistical models, existing systems lack the flexibility and efficiency needed for practical use with more challenging models arising in fields like computer vision and robotics. This paper introduces Gen, a general-purpose probabilistic programming system that achieves modeling flexibility and inference efficiency via several novel language constructs: (i) the generative function interface for encapsulating probabilistic models; (ii) interoperable modeling languages that strike different flexibility/efficiency trade-offs; (iii) combinators that exploit common patterns of conditional independence; and (iv) an inference library that empowers users to implement efficient inference algorithms at a high level of abstraction. We show that Gen outperforms state-of-the-art probabilistic programming systems, sometimes by multiple orders of magnitude, on diverse problems including object tracking, estimating 3D body pose from a depth image, and inferring the structure of a time series.},
	urldate = {2025-04-17},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Cusumano-Towner, Marco F. and Saad, Feras A. and Lew, Alexander K. and Mansinghka, Vikash K.},
	month = jun,
	year = {2019},
	pages = {221--236},
	file = {Full Text PDF:/Users/femtomc/Zotero/storage/KSCPID2Z/Cusumano-Towner et al. - 2019 - Gen a general-purpose probabilistic programming system with programmable inference.pdf:application/pdf},
}

@inproceedings{mansinghka_probabilistic_2018,
	address = {New York, NY, USA},
	series = {{PLDI} 2018},
	title = {Probabilistic programming with programmable inference},
	isbn = {978-1-4503-5698-5},
	url = {https://doi.org/10.1145/3192366.3192409},
	doi = {10.1145/3192366.3192409},
	abstract = {We introduce inference metaprogramming for probabilistic programming languages, including new language constructs, a formalism, and the rst demonstration of e ectiveness in practice. Instead of relying on rigid black-box inference algorithms hard-coded into the language implementation as in previous probabilistic programming languages, infer- ence metaprogramming enables developers to 1) dynamically decompose inference problems into subproblems, 2) apply in- ference tactics to subproblems, 3) alternate between incorpo- rating new data and performing inference over existing data, and 4) explore multiple execution traces of the probabilis- tic program at once. Implemented tactics include gradient- based optimization, Markov chain Monte Carlo, variational inference, and sequental Monte Carlo techniques. Inference metaprogramming enables the concise expression of proba- bilistic models and inference algorithms across diverse elds, such as computer vision, data science, and robotics, within a single probabilistic programming language.},
	urldate = {2025-04-18},
	booktitle = {Proceedings of the 39th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Mansinghka, Vikash K. and Schaechtle, Ulrich and Handa, Shivam and Radul, Alexey and Chen, Yutian and Rinard, Martin},
	month = jun,
	year = {2018},
	pages = {603--616},
}

@inproceedings{beutner_guaranteed_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Guaranteed bounds for posterior inference in universal probabilistic programming},
	isbn = {978-1-4503-9265-5},
	url = {https://doi.org/10.1145/3519939.3523721},
	doi = {10.1145/3519939.3523721},
	abstract = {We propose a new method to approximate the posterior distribution of probabilistic programs by means of computing guaranteed bounds. The starting point of our work is an interval-based trace semantics for a recursive, higher-order probabilistic programming language with continuous distributions.  
Taking the form of (super-/subadditive) measures, these lower/upper bounds are non-stochastic and provably correct: using the semantics, we prove that the actual posterior of a given program is sandwiched between the lower and upper bounds (soundness); moreover, the bounds converge to the posterior (completeness). As a practical and sound approximation, we introduce a weight-aware interval type system, which automatically infers interval bounds on not just the return value but also the weight of program executions, simultaneously. We have built a tool implementation, called GuBPI, which automatically computes these posterior lower/upper bounds. Our evaluation on examples from the literature shows that the bounds are useful, and can even be used to recognise wrong outputs from stochastic posterior inference procedures.},
	urldate = {2025-04-19},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Beutner, Raven and Ong, C.-H. Luke and Zaiser, Fabian},
	month = jun,
	year = {2022},
	pages = {536--551},
	file = {Submitted Version:/Users/femtomc/Zotero/storage/FMRJW48H/Beutner et al. - 2022 - Guaranteed bounds for posterior inference in universal probabilistic programming.pdf:application/pdf},
}

@article{li_compiling_2024,
	title = {Compiling {Probabilistic} {Programs} for {Variable} {Elimination} with {Information} {Flow}},
	volume = {8},
	url = {https://dl.acm.org/doi/10.1145/3656448},
	doi = {10.1145/3656448},
	abstract = {A key promise of probabilistic programming is the ability to specify rich models using an expressive program- ming language. However, the expressive power that makes probabilistic programming languages enticing also poses challenges to inference, so much so that specialized approaches to inference ban language features such as recursion. We present an approach to variable elimination and marginal inference for probabilistic programs featuring bounded recursion, discrete distributions, and sometimes continuous distributions. A compiler eliminates probabilistic side effects, using a novel information-flow type system to factorize probabilistic computations and hoist independent subcomputations out of sums or integrals. For a broad class of recursive programs with dynamically recurring substructure, the compiler effectively decomposes a global marginal-inference problem, which may otherwise be intractable, into tractable subproblems. We prove the compilation correct by showing that it preserves denotational semantics. Experiments show that the compiled programs subsume widely used PTIME algorithms for recursive models and that the compilation time scales with the size of the inference problems. As a separate contribution, we develop a denotational, logical-relations model of information-flow types in the novel measure-theoretic setting of probabilistic programming; we use it to prove noninterference and consequently the correctness of variable elimination.},
	number = {PLDI},
	urldate = {2025-04-19},
	journal = {Artifact for Paper 'Variable Elimination for an Expressive Probabilistic Programming Language'},
	author = {Li, Jianlin and Wang, Eric and Zhang, Yizhou},
	month = jun,
	year = {2024},
	pages = {218:1755--218:1780},
	file = {Full Text PDF:/Users/femtomc/Zotero/storage/MINRX23A/Li et al. - 2024 - Compiling Probabilistic Programs for Variable Elimination with Information Flow.pdf:application/pdf},
}

@article{lew_trace_2019,
	title = {Trace types and denotational semantics for sound programmable inference in probabilistic languages},
	volume = {4},
	url = {https://dl.acm.org/doi/10.1145/3371087},
	doi = {10.1145/3371087},
	abstract = {Modern probabilistic programming languages aim to formalize and automate key aspects of probabilistic modeling and inference. Many languages provide constructs for programmable inference that enable developers to improve inference speed and accuracy by tailoring an algorithm for use with a particular model or dataset. Unfortunately, it is easy to use these constructs to write unsound programs that appear to run correctly but produce incorrect results. To address this problem, we present a denotational semantics for programmable inference in higher-order probabilistic programming languages, along with a type system that ensures that well-typed inference programs are sound by construction. A central insight is that the type of a probabilistic expression can track the space of its possible execution traces, not just the type of value that it returns, as these traces are often the objects that inference algorithms manipulate. We use our semantics and type system to establish soundness properties of custom inference programs that use constructs for variational, sequential Monte Carlo, importance sampling, and Markov chain Monte Carlo inference.},
	number = {POPL},
	urldate = {2025-04-19},
	journal = {Proc. ACM Program. Lang.},
	author = {Lew, Alexander K. and Cusumano-Towner, Marco F. and Sherman, Benjamin and Carbin, Michael and Mansinghka, Vikash K.},
	month = dec,
	year = {2019},
	pages = {19:1--19:32},
	file = {Full Text PDF:/Users/femtomc/Zotero/storage/YB3HWGDL/Lew et al. - 2019 - Trace types and denotational semantics for sound programmable inference in probabilistic languages.pdf:application/pdf},
}

@article{zaiser_guaranteed_2025,
	title = {Guaranteed {Bounds} on {Posterior} {Distributions} of {Discrete} {Probabilistic} {Programs} with {Loops}},
	volume = {9},
	url = {https://dl.acm.org/doi/10.1145/3704874},
	doi = {10.1145/3704874},
	abstract = {We study the problem of bounding the posterior distribution of discrete probabilistic programs with unbounded support, loops, and conditioning. Loops pose the main difficulty in this setting: even if exact Bayesian inference is possible, the state of the art requires user-provided loop invariant templates. By contrast, we aim to find guaranteed bounds, which sandwich the true distribution. They are fully automated, applicable to more programs and provide more provable guarantees than approximate sampling-based inference. Since lower bounds can be obtained by unrolling loops, the main challenge is upper bounds, and we attack it in two ways. The first is called residual mass semantics, which is a flat bound based on the residual probability mass of a loop. The approach is simple, efficient, and has provable guarantees.  The main novelty of our work is the second approach, called geometric bound semantics. It operates on a novel family of distributions, called eventually geometric distributions (EGDs), and can bound the distribution of loops with a new form of loop invariants called contraction invariants. The invariant synthesis problem reduces to a system of polynomial inequality constraints, which is a decidable problem with automated solvers. If a solution exists, it yields an exponentially decreasing bound on the whole distribution, and can therefore bound moments and tail asymptotics as well, not just probabilities as in the first approach.  Both semantics enjoy desirable theoretical properties. In particular, we prove soundness and convergence, i.e. the bounds converge to the exact posterior as loops are unrolled further. We also investigate sufficient and necessary conditions for the existence of geometric bounds. On the practical side, we describe Diabolo, a fully-automated implementation of both semantics, and evaluate them on a variety of benchmarks from the literature, demonstrating their general applicability and the utility of the resulting bounds.},
	number = {POPL},
	urldate = {2025-04-19},
	journal = {Artifact for: Guaranteed Bounds on Posterior Distributions of Discrete Probabilistic Programs with Loops (POPL 2025)},
	author = {Zaiser, Fabian and Murawski, Andrzej S. and Ong, C.-H. Luke},
	month = jan,
	year = {2025},
	pages = {38:1104--38:1135},
	file = {Full Text PDF:/Users/femtomc/Zotero/storage/C9CGXL59/Zaiser et al. - 2025 - Guaranteed Bounds on Posterior Distributions of Discrete Probabilistic Programs with Loops.pdf:application/pdf},
}
